{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.ops import unary_union\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def download(des_loc,\n",
    "            http_page,\n",
    "            str1,\n",
    "            str2):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a http and two str in the name of the link and save them in\n",
    "    provided destnation\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    des_loc: string, the name of the source file including path and extension\n",
    "    http_page: string, the name of the corresponding catchment (subbasin) for the unresolved hills\n",
    "    str1: string, a part of the link name to filter\n",
    "    str2: string, a second part of the link name to filter\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \"\"\"\n",
    "\n",
    "    # first get all the links in the page\n",
    "    req = Request(http_page)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    # specify the link to be downloaded\n",
    "    link_to_download = []\n",
    "    for link in links:\n",
    "        # if \"hillslope\" in link and \"clean\" in link: # links that have cat_pfaf and Basins in them\n",
    "        if str1 in link and str2 in link: # links that have cat_pfaf and Basins in them\n",
    "            link_to_download.append(link)\n",
    "            print(link)\n",
    "\n",
    "    # creat urls to download\n",
    "    urls =[]\n",
    "    for file_name in link_to_download:\n",
    "        urls.append(http_page+file_name) # link of the page + file names\n",
    "        print(http_page+file_name)\n",
    "    print(urls)\n",
    "\n",
    "    # loop to download the data\n",
    "    for url in urls:\n",
    "        name = url.split('/')[-1] # get the name of the file at the end of the url to download\n",
    "        r = requests.get(url) # download the URL\n",
    "        # print the specification of the download \n",
    "        print(r.status_code, r.headers['content-type'], r.encoding)\n",
    "        # if download successful the statuse code is 200 then save the file, else print not downloaded\n",
    "        if r.status_code == 200:\n",
    "            print('download was successful for '+url)\n",
    "            with open(des_loc+name, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        else:\n",
    "            print('download was not successful for '+url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download section and input path\n",
    "### A list of IDs based on 2 digit pfaf code are provided for download, the path to save the donwload is provided and also the website to download the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 2 digit pfaf code for the shapefile to be processed\n",
    "# list of IDs for downloading the processing\n",
    "IDs = ['11', '12', '13', '14', '15', '16', '17', '18',\n",
    "       '21', '22', '23', '24', '25', '26', '27', '28', '29',\n",
    "       '31', '32', '33', '34', '35', '36',\n",
    "       '41', '42', '43', '44', '45', '46', '47', '48', '49',\n",
    "       '51', '52', '53', '54', '55', '56', '57',\n",
    "       '61', '62', '63', '64', '65', '66', '67',\n",
    "       '71', '72', '73', '74', '75', '76', '77', '78',\n",
    "       '81', '82', '83', '84', '85', '86',\n",
    "       '91']\n",
    "# location of files online\n",
    "http_path = 'XXX/for_martyn/' # link to the page that the data exists\n",
    "# in this folder create subfolders cat, riv, hill, cat_step_1,cat_step_2\n",
    "path = '/Users/shg096/Desktop/MERIT_Hydro/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP- prepare the folder and subfolders for download\n",
    "# path is the location were all the shapefiles and anupulaed shapfiles are saved\n",
    "# under path create five subfolders: cat, riv, hill, cat_step_0, cat_step_1, cat_fixed, hill_fixed\n",
    "if not os.path.exists(path+'cat'):\n",
    "    os.mkdir(path+'cat')\n",
    "if not os.path.exists(path+'riv'):\n",
    "    os.mkdir(path+'riv')\n",
    "if not os.path.exists(path+'hill'):\n",
    "    os.mkdir(path+'hill')\n",
    "if not os.path.exists(path+'cat_step_0'):\n",
    "    os.mkdir(path+'cat_step_0')\n",
    "if not os.path.exists(path+'cat_step_1'):\n",
    "    os.mkdir(path+'cat_step_1')\n",
    "if not os.path.exists(path+'cat_fixed'):\n",
    "    os.mkdir(path+'cat_fixed')\n",
    "if not os.path.exists(path+'ERA5int'):\n",
    "    os.mkdir(path+'ERA5int')\n",
    "if not os.path.exists(path+'hill_fixed'):\n",
    "    os.mkdir(path+'hill_fixed')\n",
    "if not os.path.exists(path+'hill_step_0'):\n",
    "    os.mkdir(path+'hill_step_0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the catchment, river, costal hillslope\n",
    "for ID in IDs:\n",
    "    download(path+'cat/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'cat',\n",
    "        ID)\n",
    "    download(path+'riv/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'riv',\n",
    "        ID)\n",
    "    if ID != '49' # there is no hillslope for 49\n",
    "        download(path+'hill/',\n",
    "             http_path+'coastal_hillslopes/',\n",
    "            'hill',\n",
    "            ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
