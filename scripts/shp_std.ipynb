{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.ops import unary_union\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def remove_dublicate_point(poly):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "    \n",
    "    This function gets a shapely polygon and remove dublicated point there\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    poly: shapely polygon, the input shapely polygon\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \n",
    "    \"\"\"\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    X = np.transpose(x)\n",
    "    Y = np.transpose(y)\n",
    "    df = pd.DataFrame({'X': X, 'Y': Y})\n",
    "    df['X_next'] = df['X']\n",
    "    df['Y_next'] = df['Y']\n",
    "    df['X_next'] = df['X_next'].shift(-1)\n",
    "    df['Y_next'] = df['Y_next'].shift(-1)\n",
    "    df['dist_next'] = 0\n",
    "    df['dist_next'] = ((df.X-df.X_next)**2+(df.Y-df.Y_next)**2)**0.5\n",
    "    df['X_before'] = df['X']\n",
    "    df['Y_before'] = df['Y']\n",
    "    df['X_before'] = df['X_before'].shift(1)\n",
    "    df['Y_before'] = df['Y_before'].shift(1)\n",
    "    df['dist_before'] = 0\n",
    "    df['dist_before'] = ((df.X-df.X_before)**2+(df.Y-df.Y_before)**2)**0.5\n",
    "    df = df[(df['dist_next']>0.0007) | (df['dist_before']>0.0007)] # remove the point that are very close to each other less than 0.00083\n",
    "    df = df.drop(columns=['X_next', 'Y_next', 'dist_next', 'X_before', 'Y_before', 'dist_before'])\n",
    "    print(df.shape[0])\n",
    "    df['X'] = np.floor((df['X']*100000))/100000\n",
    "    df['Y'] = np.floor((df['X']*100000))/100000\n",
    "    df = df.drop_duplicates() # remove dublication of points\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['index'])\n",
    "    df = pd.concat([df, df.head(1)]) # close the polygone\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns=['index'])\n",
    "    print(df.shape[0])\n",
    "    df['geometry'] = df.apply(lambda row: Point(row.X, row.Y), axis=1) # create a geometry field\n",
    "    poly_result = Polygon([(p.x, p.y)  for p in  df.geometry]) # make the polygon\n",
    "    return poly_result\n",
    "\n",
    "\n",
    "def download(des_loc,\n",
    "            http_page,\n",
    "            str1,\n",
    "            str2):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a http and two str in the name of the link and save them in\n",
    "    provided destnation\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    des_loc: string, the name of the source file including path and extension\n",
    "    http_page: string, the name of the corresponding catchment (subbasin) for the unresolved hills\n",
    "    str1: string, a part of the link name to filter\n",
    "    str2: string, a second part of the link name to filter\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \"\"\"\n",
    "\n",
    "    # first get all the links in the page\n",
    "    req = Request(http_page)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    # specify the link to be downloaded\n",
    "    link_to_download = []\n",
    "    for link in links:\n",
    "        # if \"hillslope\" in link and \"clean\" in link: # links that have cat_pfaf and Basins in them\n",
    "        if str1 in link and str2 in link: # links that have cat_pfaf and Basins in them\n",
    "            link_to_download.append(link)\n",
    "            print(link)\n",
    "\n",
    "    # creat urls to download\n",
    "    urls =[]\n",
    "    for file_name in link_to_download:\n",
    "        urls.append(http_page+file_name) # link of the page + file names\n",
    "        print(http_page+file_name)\n",
    "    print(urls)\n",
    "\n",
    "    # loop to download the data\n",
    "    for url in urls:\n",
    "        name = url.split('/')[-1] # get the name of the file at the end of the url to download\n",
    "        r = requests.get(url) # download the URL\n",
    "        # print the specification of the download \n",
    "        print(r.status_code, r.headers['content-type'], r.encoding)\n",
    "        # if download successful the statuse code is 200 then save the file, else print not downloaded\n",
    "        if r.status_code == 200:\n",
    "            print('download was successful for '+url)\n",
    "            with open(des_loc+name, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        else:\n",
    "            print('download was not successful for '+url)\n",
    "\n",
    "\n",
    "def shp_hill (name_of_source_file, name_of_cat_file, name_of_result_file, epsilon):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile and remove inernal holes\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_cat_file: string, the name of the corresponding catchment (subbasin)\n",
    "        for the unresolved hills\n",
    "    name_of_result_file: string, the name of the file that includes fixed shapes\n",
    "        including path and extension\n",
    "    epsilon\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the fixed shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    shp = gpd.read_file(name_of_source_file)\n",
    "    cat1 = gpd.read_file(name_of_cat_file)\n",
    "    shp_all = None\n",
    "\n",
    "\n",
    "    ## STEP1, load a shapefile, and find its intesection with itself. there are\n",
    "    ## there should be some holes in the shapefile. the holes are given as a separaete shape\n",
    "\n",
    "    shp_temp = gpd.overlay(shp, shp, how='intersection')\n",
    "    shp_temp = shp_temp [shp_temp.FID_1 != shp_temp.FID_2]\n",
    "    shp_temp = shp_temp.reset_index()\n",
    "    \n",
    "    shp_temp = gpd.overlay(shp, shp_temp, how='difference')\n",
    "    shp_temp = shp_temp.reset_index()\n",
    "    \n",
    "    ## STEP2, remove possible cat from the unresolved costal hillslope\n",
    "    shp_temp = gpd.overlay(shp_temp, cat1, how='difference')\n",
    "    shp_temp = shp_temp.reset_index()\n",
    "    \n",
    "    ## STEP3, break the touching polygons into separate polygons, remove the links (lines)\n",
    "    shp_temp = shp_temp.buffer(-epsilon).buffer(epsilon)\n",
    "    shp_temp = gpd.GeoDataFrame(shp_temp)\n",
    "    shp_temp.columns = ['geometry'] # rename the colomn to geometry\n",
    "    shp_temp.to_file('temp4.shp')\n",
    "\n",
    "    ## STEP4, break the polygones into separete shape in a shapefile\n",
    "    shp = gpd.read_file('temp4.shp')\n",
    "\n",
    "    for index, _ in shp.iterrows():\n",
    "        \n",
    "        polys = shp.geometry.iloc[index] # get the shape\n",
    "        \n",
    "        if polys is not None:\n",
    "            \n",
    "            if polys.type is 'Polygon':\n",
    "                shp_temp = gpd.GeoSeries(polys) # convert multipolygon to a shapefile with polygons only\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # convert multipolygon to a shapefile with polygons\n",
    "                shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            if polys.type is 'MultiPolygon':\n",
    "                shp_temp = gpd.GeoDataFrame(polys) # convert multipolygon to a shapefile with polygons \n",
    "                shp_temp.columns = ['geometry'] # naming geometry column\n",
    "\n",
    "            if shp_all is None:\n",
    "                shp_all = shp_temp\n",
    "            else:\n",
    "                shp_all = shp_all.append(shp_temp)\n",
    "\n",
    "#     for index, _ in shp_all.iterrows(): # assuming the code convert everything to polygone (and not multi)\n",
    "#         poly = shp_all.geometry.iloc[index]\n",
    "#         poly = remove_dublicate_point (poly)\n",
    "#         shp_all.geometry.iloc[index] = poly\n",
    "\n",
    "    shp_all.to_file(name_of_result_file)\n",
    "\n",
    "def extract_poly_coords(geom):\n",
    "    if geom.type == 'Polygon':\n",
    "        exterior_coords = geom.exterior.coords[:]\n",
    "        interior_coords = []\n",
    "        for interior in geom.interiors:\n",
    "            interior_coords += interior.coords[:]\n",
    "    elif geom.type == 'MultiPolygon':\n",
    "        exterior_coords = []\n",
    "        interior_coords = []\n",
    "        for part in geom:\n",
    "            epc = extract_poly_coords(part)  # Recursive call\n",
    "            exterior_coords += epc['exterior_coords']\n",
    "            interior_coords += epc['interior_coords']\n",
    "    else:\n",
    "        raise ValueError('Unhandled geometry type: ' + repr(geom.type))\n",
    "    return {'exterior_coords': exterior_coords,\n",
    "            'interior_coords': interior_coords}\n",
    "\n",
    "\n",
    "def shp_std_light(name_of_source_file,\n",
    "              name_of_result_file,\n",
    "              name_of_result_file_fixed_shapes,\n",
    "              ID_field,\n",
    "              epsilon,\n",
    "              list_id):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile and remove inernal holes\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_result_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file_fixed_shapes: string, the name of the file that includes fixed shapes\n",
    "        including path and extension\n",
    "    name_of_log_file: string, the name of the text log file with path and txt extension\n",
    "    ID_field: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    epsilon: real, the minimum distance for buffer operation\n",
    "    list_id: list of shape IDs that should be corrected\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the fixed shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the shapefile\n",
    "    shp = gpd.read_file(name_of_source_file)\n",
    "    shp_new = shp # pass the shape to a new shape\n",
    "    shp_new['flag'] = 0 # add flag for the shapefile ids that are resolved\n",
    "    \n",
    "    for ID in list_id:\n",
    "        for index, _ in shp.iterrows():\n",
    "            if shp[ID_field][index] == ID:\n",
    "                shp_temp = shp.geometry.iloc[index]\n",
    "                shp_temp = shp_temp.buffer(epsilon) # to amalgamate tmultipolygons into a polygon\n",
    "                shp_temp = gpd.GeoSeries(shp_temp) # to geoseries\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # to geoframe\n",
    "                shp_temp.columns = ['geometry'] # call the colomn geometry\n",
    "                poly = shp_temp.geometry.iloc[0] # get the polygone from the shapefile\n",
    "                A = extract_poly_coords(poly) # extract the exterior\n",
    "                outer = A['exterior_coords'] # pass the exterior\n",
    "                poly_new = Polygon (outer) # make a polygone out of the \n",
    "                shp_new.geometry.iloc[index] = poly_new # pass the geometry to the new shapefile\n",
    "                shp_new['flag'].iloc[index] = 1 # put flag as 1\n",
    "                shp_temp = shp_new.geometry.iloc[index] # get the shape\n",
    "                shp_temp = shp_temp.buffer(-epsilon) # redo the buffer\n",
    "                shp_temp = gpd.GeoSeries(shp_temp) # to geoseries\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # geo dataframe\n",
    "                shp_temp.columns = ['geometry'] # name the column as geometry\n",
    "                shp_new.geometry.iloc[index] = shp_temp.geometry.iloc[0] # pass that to the new shape\n",
    "#                 poly = shp_new.geometry.iloc[index]\n",
    "#                 poly = remove_dublicate_point (poly)\n",
    "#                 shp_new.geometry.iloc[index] = poly\n",
    "    \n",
    "    shp_new.to_file(name_of_result_file) \n",
    "    shp_new = shp_new [shp_new.flag ==1]\n",
    "    if not shp_new.empty:\n",
    "        shp_new.to_file(name_of_result_file_fixed_shapes)\n",
    "    \n",
    "#     shp_new = shp_new [shp_new.flag ==1] # get the shapes that flag are 1\n",
    "#     if not shp_new.empty:\n",
    "#         shp_new = shp_new.drop(columns=['flag'])\n",
    "#         shp_diff = gpd.overlay(shp, shp_new, how='difference') # get the difference\n",
    "#         shp_all = shp_new.append(shp_diff) # append the fixed shapefiles to the diff\n",
    "#         if shp.shape[0] == shp_all.shape[0]:\n",
    "#             shp_new.to_file(name_of_result_file_fixed_shapes) #saved the fixed shapefile\n",
    "#             shp_all.to_file(name_of_result_file) # save the entire\n",
    "#         else:\n",
    "#             print('input output have different lenght; check')\n",
    "\n",
    "def shp_std_hard(name_of_source_file,\n",
    "            name_of_result_file,\n",
    "            name_of_result_file_holes,\n",
    "            name_of_log_file,\n",
    "            ID_field,\n",
    "            area_tolerance):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile, its directory, and its extensions (such as gpkg or shp) and\n",
    "    save a stadard shapefile. if presence it also save the holes of a shapefile\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_result_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file_holes: string, the name of the file that includes holes including path\n",
    "        and extension\n",
    "    name_of_log_file: string, the name of the text log file with path and txt extension\n",
    "    ID_field: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    area_tolerance: float; the tolerance to compare area before and after correction and report\n",
    "        differences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the removed problematice holes\n",
    "    a log file in the same folder descringin the invalid shapefiles\n",
    "    \"\"\"\n",
    "\n",
    "    shp_original = gpd.read_file(name_of_source_file)\n",
    "    shp_poly     = shp_original\n",
    "    shp_hole     = None\n",
    "\n",
    "    logfile = open(name_of_log_file,\"w\") # preparing the log file to write\n",
    "\n",
    "    number_invalid = 0 # counter for invalid shapes\n",
    "    number_resolved = 0 # counter for resolved invalid shapes\n",
    "    number_not_resolved = 0 # counter for not resolved invalid shapes\n",
    "\n",
    "    for index, _ in shp_original.iterrows():\n",
    "\n",
    "        # initialization\n",
    "        polys = shp_original.geometry.iloc[index] # get the shape\n",
    "        area_before = polys.area # area before changes\n",
    "        invalid = False # initializing invalid as false\n",
    "        \n",
    "        # check if the shapefile is valid\n",
    "        if polys.is_valid is False: # check if the geometry is invalid\n",
    "            number_invalid = number_invalid + 1\n",
    "            invalid = True\n",
    "            str_temp = str(number_invalid)+\". shape with ID \"+str(shp_original[ID_field].iloc[index])+\\\n",
    "            \" is not valid\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "        # put the shape into a Polygon or MultiPolygon\n",
    "        if polys.type is 'Polygon':\n",
    "            # print(polys.type)\n",
    "            shp_temp = gpd.GeoSeries(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            #shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            shp_temp = gpd.GeoDataFrame(shp_temp) # convert multipolygon to a shapefile with polygons\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            #print(shp_temp)\n",
    "        if polys.type is 'MultiPolygon':\n",
    "            # print(polys.type)\n",
    "            shp_temp = gpd.GeoDataFrame(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            #print(shp_temp)\n",
    "\n",
    "        has_holes = False # initializing hole as false\n",
    "        shp_temp['CCW'] = 0 # initialize check for couterclockwise (holes)\n",
    "        for index1, _ in shp_temp.iterrows(): #loop over polygone of one element\n",
    "            poly = shp_temp.geometry.iloc[index1] # get the geometry of polygon\n",
    "            if poly.exterior.is_ccw is True: # then the polgone is a hole\n",
    "                shp_temp['CCW'].iloc[index1] = 1 # set the hole flag to 1\n",
    "                shp_temp['geometry'].iloc[index1] = shapely.geometry.polygon.orient(poly, sign = +1) \n",
    "                # +1 CCW\n",
    "                #print(shp_temp['geometry'].iloc[index1])\n",
    "                has_holes = True\n",
    "\n",
    "        shp_temp_polys = shp_temp[shp_temp.CCW ==0] # get the polyons that are not couter clockwise\n",
    "        shp_temp_polys['dis'] = 0 # add a field for desolve\n",
    "        shp_temp_polys = shp_temp_polys.dissolve(by='dis') # to one multipolygon\n",
    "        polys_temp = shp_temp_polys.geometry.iloc[0] # update the shapefile on that\n",
    "        polys_temp = unary_union(polys_temp) # unify all the polygons into a multipolygons\n",
    "        shp_poly.geometry.iloc[index] = polys_temp.buffer(0) # fix the issue by buffer(0)\n",
    "        area_after = shp_poly.geometry.iloc[index].area # area after changes\n",
    "\n",
    "        # check if the shapefile becomes valid\n",
    "        # check if the geometry #is invalid\n",
    "        if shp_poly.geometry.iloc[index].is_valid is True and invalid is True: \n",
    "            str_temp = \" and becomes valid \\n\"\n",
    "            logfile.write(str_temp)\n",
    "            number_resolved = number_resolved + 1\n",
    "        # check if the geometry is invalid\n",
    "        if shp_poly.geometry.iloc[index].is_valid is False and invalid is True:\n",
    "            str_temp = \" and does not become valid; please check the shape \\n\"\n",
    "            logfile.write(str_temp)\n",
    "            number_not_resolved = number_not_resolved + 1\n",
    "\n",
    "        if has_holes is True:\n",
    "            shp_temp_holes = shp_temp[shp_temp.CCW ==1]\n",
    "            shp_temp_holes['dis'] = 0\n",
    "            shp_temp_holes = shp_temp_holes.dissolve(by='dis') # to one multipolyno\n",
    "            shp_temp_holes[ID_field] = shp_original[ID_field].iloc[index]\n",
    "            if shp_hole is None:\n",
    "                shp_hole = shp_temp_holes\n",
    "            else:\n",
    "                shp_hole = gpd.GeoDataFrame( pd.concat([shp_hole, shp_temp_holes], ignore_index=True) )\n",
    "            str_temp = \"Shape has a hole \\n\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "        if abs(area_before-area_after)>area_tolerance: # tolernace can be different based on projection\n",
    "            str_temp = \"shape area changes abs(\"+str(area_before)+\"-\"+str(area_after)+\") = \"+\\\n",
    "            str(area_before-area_after)+\" \\n\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "\n",
    "    shp_poly.to_file(name_of_result_file)\n",
    "    if shp_hole is not None:\n",
    "        shp_hole.to_file(name_of_result_file_holes) #save any hole to check\n",
    "\n",
    "    str_temp = \"Total number of shapes = \"+str(shp_original.shape[0])+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of invalid shapes = \"+str(number_invalid)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of resolved invalid shapes = \"+str(number_resolved)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of not resolved invalid shapes = \"+str(number_not_resolved)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    logfile.close() # close the log gile\n",
    "\n",
    "def intersection_shp(shp_1, shp_2):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/candex\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  Apache2\n",
    "    This fucntion intersect two shapefile. It keeps the fiels from the first and second shapefiles (identified by S_1_ and \n",
    "    S_2_). It also creats other field including AS1 (area of the shape element from shapefile 1), IDS1 (an arbitary index\n",
    "    for the shapefile 1), AS2 (area of the shape element from shapefile 1), IDS2 (an arbitary index for the shapefile 1), \n",
    "    AINT (the area of teh intersected shapes), AP1 (the area of the intersected shape to the shapes from shapefile 1),\n",
    "    AP2 (the area of teh intersected shape to the shapefes from shapefile 2), AP1N (the area normalized in the case AP1\n",
    "    summation is not 1 for a given shape from shapefile 1, this will help to preseve mass if part of the shapefile are not \n",
    "    intersected), AP2N (the area normalized in the case AP2 summation is not 1 for a given shape from shapefile 2, this\n",
    "    will help to preseve mass if part of the shapefile are not intersected)\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    shp1: geo data frame, shapefile 1\n",
    "    shp2: geo data frame, shapefile 2\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result: a geo data frame that includes the intersected shapefile and area, percent and normalized percent of each shape\n",
    "    elements in another one\n",
    "    \"\"\"\n",
    "    # Calculating the area of every shapefile (both should be in degree or meters)\n",
    "    column_names = shp_1.columns\n",
    "    column_names = list(column_names)\n",
    "\n",
    "    # removing the geometry from the column names\n",
    "    column_names.remove('geometry')\n",
    "\n",
    "    # renaming the column with S_1\n",
    "    for i in range(len(column_names)):\n",
    "        shp_1 = shp_1.rename(\n",
    "            columns={column_names[i]: 'S_1_' + column_names[i]})\n",
    "\n",
    "    column_names = shp_2.columns\n",
    "    column_names = list(column_names)\n",
    "\n",
    "    # removing the geometry from the colomn names\n",
    "    column_names.remove('geometry')\n",
    "\n",
    "    # renaming the column with S_2\n",
    "    for i in range(len(column_names)):\n",
    "        shp_2 = shp_2.rename(\n",
    "            columns={column_names[i]: 'S_2_' + column_names[i]})\n",
    "\n",
    "    # Caclulating the area for shp1\n",
    "    shp_1['AS1'] = shp_1.area\n",
    "    shp_1['IDS1'] = np.arange(shp_1.shape[0])+1\n",
    "\n",
    "    # Caclulating the area for shp2\n",
    "    shp_2['AS2'] = shp_2.area\n",
    "    shp_2['IDS2'] = np.arange(shp_2.shape[0])+1\n",
    "\n",
    "    # making intesection\n",
    "    result = spatial_overlays (shp_1, shp_2, how='intersection')\n",
    "\n",
    "    # Caclulating the area for shp2\n",
    "    result['AINT'] = result['geometry'].area\n",
    "    result['AP1'] = result['AINT']/result['AS1']\n",
    "    result['AP2'] = result['AINT']/result['AS2']\n",
    "    \n",
    "    \n",
    "    # taking the part of data frame as the numpy to incread the spead\n",
    "    # finding the IDs from shapefile one\n",
    "    ID_S1 = np.array (result['IDS1'])\n",
    "    AP1 = np.array(result['AP1'])\n",
    "    AP1N = AP1 # creating the nnormalized percent area\n",
    "    ID_S1_unique = np.unique(ID_S1) #unique idea\n",
    "    for i in ID_S1_unique:\n",
    "        INDX = np.where(ID_S1==i) # getting the indeces\n",
    "        AP1N[INDX] = AP1[INDX] / AP1[INDX].sum() # normalizing for that sum\n",
    "        \n",
    "    # taking the part of data frame as the numpy to incread the spead\n",
    "    # finding the IDs from shapefile one\n",
    "    ID_S2 = np.array (result['IDS2'])\n",
    "    AP2 = np.array(result['AP2'])\n",
    "    AP2N = AP2 # creating the nnormalized percent area\n",
    "    ID_S2_unique = np.unique(ID_S2) #unique idea\n",
    "    for i in ID_S2_unique:\n",
    "        INDX = np.where(ID_S2==i) # getting the indeces\n",
    "        AP2N[INDX] = AP2[INDX] / AP2[INDX].sum() # normalizing for that sum\n",
    "        \n",
    "    result ['AP1N'] = AP1N\n",
    "    result ['AP2N'] = AP2N\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def spatial_overlays(df1, df2, how='intersection', reproject=True):\n",
    "    \"\"\"Perform spatial overlay between two polygons.\n",
    "    Currently only supports data GeoDataFrames with polygons.\n",
    "    Implements several methods that are all effectively subsets of\n",
    "    the union.\n",
    "    \n",
    "    Omer Ozak\n",
    "    ozak\n",
    "    https://github.com/ozak\n",
    "    https://github.com/geopandas/geopandas/pull/338\n",
    "    Parameters\n",
    "    ----------\n",
    "    df1 : GeoDataFrame with MultiPolygon or Polygon geometry column\n",
    "    df2 : GeoDataFrame with MultiPolygon or Polygon geometry column\n",
    "    how : string\n",
    "        Method of spatial overlay: 'intersection', 'union',\n",
    "        'identity', 'symmetric_difference' or 'difference'.\n",
    "    use_sindex : boolean, default True\n",
    "        Use the spatial index to speed up operation if available.\n",
    "    Returns\n",
    "    -------\n",
    "    df : GeoDataFrame\n",
    "        GeoDataFrame with new set of polygons and attributes\n",
    "        resulting from the overlay\n",
    "    \"\"\"\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    df1['geometry'] = df1.geometry.buffer(0)\n",
    "    df2['geometry'] = df2.geometry.buffer(0)\n",
    "    if df1.crs!=df2.crs and reproject:\n",
    "        print('Data has different projections.')\n",
    "        print('Converted data to projection of first GeoPandas DatFrame')\n",
    "        df2.to_crs(crs=df1.crs, inplace=True)\n",
    "    if how=='intersection':\n",
    "        # Spatial Index to create intersections\n",
    "        spatial_index = df2.sindex\n",
    "        df1['bbox'] = df1.geometry.apply(lambda x: x.bounds)\n",
    "        df1['sidx']=df1.bbox.apply(lambda x:list(spatial_index.intersection(x)))\n",
    "        pairs = df1['sidx'].to_dict()\n",
    "        nei = []\n",
    "        for i,j in pairs.items():\n",
    "            for k in j:\n",
    "                nei.append([i,k])\n",
    "        pairs = gpd.GeoDataFrame(nei, columns=['idx1','idx2'], crs=df1.crs)\n",
    "        pairs = pairs.merge(df1, left_on='idx1', right_index=True)\n",
    "        pairs = pairs.merge(df2, left_on='idx2', right_index=True, suffixes=['_1','_2'])\n",
    "        pairs['Intersection'] = pairs.apply(lambda x: (x['geometry_1'].intersection(x['geometry_2'])).buffer(0), axis=1)\n",
    "        pairs = gpd.GeoDataFrame(pairs, columns=pairs.columns, crs=df1.crs)\n",
    "        cols = pairs.columns.tolist()\n",
    "        cols.remove('geometry_1')\n",
    "        cols.remove('geometry_2')\n",
    "        cols.remove('sidx')\n",
    "        cols.remove('bbox')\n",
    "        cols.remove('Intersection')\n",
    "        dfinter = pairs[cols+['Intersection']].copy()\n",
    "        dfinter.rename(columns={'Intersection':'geometry'}, inplace=True)\n",
    "        dfinter = gpd.GeoDataFrame(dfinter, columns=dfinter.columns, crs=pairs.crs)\n",
    "        dfinter = dfinter.loc[dfinter.geometry.is_empty==False]\n",
    "        dfinter.drop(['idx1','idx2'], inplace=True, axis=1)\n",
    "        return dfinter\n",
    "    elif how=='difference':\n",
    "        spatial_index = df2.sindex\n",
    "        df1['bbox'] = df1.geometry.apply(lambda x: x.bounds)\n",
    "        df1['sidx']=df1.bbox.apply(lambda x:list(spatial_index.intersection(x)))\n",
    "        df1['new_g'] = df1.apply(lambda x: reduce(lambda x, y: x.difference(y).buffer(0), \n",
    "                                 [x.geometry]+list(df2.iloc[x.sidx].geometry)) , axis=1)\n",
    "        df1.geometry = df1.new_g\n",
    "        df1 = df1.loc[df1.geometry.is_empty==False].copy()\n",
    "        df1.drop(['bbox', 'sidx', 'new_g'], axis=1, inplace=True)\n",
    "        return df1\n",
    "    elif how=='symmetric_difference':\n",
    "        df1['idx1'] = df1.index.tolist()\n",
    "        df2['idx2'] = df2.index.tolist()\n",
    "        df1['idx2'] = np.nan\n",
    "        df2['idx1'] = np.nan\n",
    "        dfsym = df1.merge(df2, on=['idx1','idx2'], how='outer', suffixes=['_1','_2'])\n",
    "        dfsym['geometry'] = dfsym.geometry_1\n",
    "        dfsym.loc[dfsym.geometry_2.isnull()==False, 'geometry'] = dfsym.loc[dfsym.geometry_2.isnull()==False, 'geometry_2']\n",
    "        dfsym.drop(['geometry_1', 'geometry_2'], axis=1, inplace=True)\n",
    "        dfsym = gpd.GeoDataFrame(dfsym, columns=dfsym.columns, crs=df1.crs)\n",
    "        spatial_index = dfsym.sindex\n",
    "        dfsym['bbox'] = dfsym.geometry.apply(lambda x: x.bounds)\n",
    "        dfsym['sidx'] = dfsym.bbox.apply(lambda x:list(spatial_index.intersection(x)))\n",
    "        dfsym['idx'] = dfsym.index.values\n",
    "        dfsym.apply(lambda x: x.sidx.remove(x.idx), axis=1)\n",
    "        dfsym['new_g'] = dfsym.apply(lambda x: reduce(lambda x, y: x.difference(y).buffer(0), \n",
    "                         [x.geometry]+list(dfsym.iloc[x.sidx].geometry)) , axis=1)\n",
    "        dfsym.geometry = dfsym.new_g\n",
    "        dfsym = dfsym.loc[dfsym.geometry.is_empty==False].copy()\n",
    "        dfsym.drop(['bbox', 'sidx', 'idx', 'idx1','idx2', 'new_g'], axis=1, inplace=True)\n",
    "        return dfsym\n",
    "    elif how=='union':\n",
    "        dfinter = spatial_overlays(df1, df2, how='intersection')\n",
    "        dfsym = spatial_overlays(df1, df2, how='symmetric_difference')\n",
    "        dfunion = dfinter.append(dfsym)\n",
    "        dfunion.reset_index(inplace=True, drop=True)\n",
    "        return dfunion\n",
    "    elif how=='identity':\n",
    "        dfunion = spatial_overlays(df1, df2, how='union')\n",
    "        cols1 = df1.columns.tolist()\n",
    "        cols2 = df2.columns.tolist()\n",
    "        cols1.remove('geometry')\n",
    "        cols2.remove('geometry')\n",
    "        cols2 = set(cols2).intersection(set(cols1))\n",
    "        cols1 = list(set(cols1).difference(set(cols2)))\n",
    "        cols2 = [col+'_1' for col in cols2]\n",
    "        dfunion = dfunion[(dfunion[cols1+cols2].isnull()==False).values]\n",
    "        return dfunion\n",
    "\n",
    "def intersect(name_of_source_file,\n",
    "            name_of_ERA_file,\n",
    "            name_of_result_file,\n",
    "            field_ID,\n",
    "            dic_rename):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile, its directory, and its extensions (such as gpkg or shp) and\n",
    "    save a stadard shapefile. if presence it also save the holes of a shapefile\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_ERA_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file: string, the name of the file that includes holes including path\n",
    "        and extension\n",
    "    field_ID: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    dic_rename: float; the tolerance to compare area before and after correction and report\n",
    "        differences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the removed problematice holes\n",
    "    a log file in the same folder descringin the invalid shapefiles\n",
    "    \"\"\"\n",
    "    \n",
    "    shp1 = gpd.read_file (name_of_source_file)\n",
    "    shp1.crs = 'epsg:4326'\n",
    "    shp1[\"lon_c\"] = shp1.centroid.x # pass calculated centroid lon to the shp1\n",
    "    shp1[\"lat_c\"] = shp1.centroid.y # pass calculated centroid lat to the shp1\n",
    "    shp2 = gpd.read_file (name_of_ERA_file)\n",
    "    shp_int = intersection_shp(shp1, shp2)\n",
    "    shp_int = shp_int.rename(columns=dic_rename) # weight of each ERA5 grid in subbasin\n",
    "    shp_int = shp_int.sort_values(by=[field_ID])\n",
    "    shp_int.to_file(name_of_result_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download section and input path\n",
    "### A list of IDs based on 2 digit pfaf code are provided for download, the path to save the donwload is provided and also the website to download the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 2 digit pfaf code for the shapefile to be processed\n",
    "# list of IDs for downloading the processing\n",
    "IDs = ['11', '12', '13', '14', '15', '16', '17', '18',\n",
    "       '21', '22', '23', '24', '25', '26', '27', '28', '29',\n",
    "       '31', '32', '33', '34', '35', '36',\n",
    "       '41', '42', '43', '44', '45', '46', '47', '48', '49',\n",
    "       '51', '52', '53', '54', '55', '56', '57',\n",
    "       '61', '62', '63', '64', '65', '66', '67',\n",
    "       '71', '72', '73', '74', '75', '76', '77', '78',\n",
    "       '81', '82', '83', '84', '85', '86',\n",
    "       '91']\n",
    "# location of files online\n",
    "http_path = 'XXX/for_martyn/' # link to the page that the data exists\n",
    "# in this folder create subfolders cat, riv, hill, cat_step_1,cat_step_2\n",
    "path = '/Users/shg096/Desktop/MERIT_Hydro/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP- prepare the folder and subfolders for download\n",
    "# path is the location were all the shapefiles and anupulaed shapfiles are saved\n",
    "# under path create five subfolders: cat, riv, hill, cat_step_0, cat_step_1, cat_fixed, hill_fixed\n",
    "if not os.path.exists(path+'cat'):\n",
    "    os.mkdir(path+'cat')\n",
    "if not os.path.exists(path+'riv'):\n",
    "    os.mkdir(path+'riv')\n",
    "if not os.path.exists(path+'hill'):\n",
    "    os.mkdir(path+'hill')\n",
    "if not os.path.exists(path+'cat_step_0'):\n",
    "    os.mkdir(path+'cat_step_0')\n",
    "if not os.path.exists(path+'cat_step_1'):\n",
    "    os.mkdir(path+'cat_step_1')\n",
    "if not os.path.exists(path+'cat_fixed'):\n",
    "    os.mkdir(path+'cat_fixed')\n",
    "if not os.path.exists(path+'ERA5int'):\n",
    "    os.mkdir(path+'ERA5int')\n",
    "if not os.path.exists(path+'hill_fixed'):\n",
    "    os.mkdir(path+'hill_fixed')\n",
    "if not os.path.exists(path+'hill_step_0'):\n",
    "    os.mkdir(path+'hill_step_0')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the catchment, river, costal hillslope\n",
    "for ID in IDs:\n",
    "    download(path+'cat/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'cat',\n",
    "        ID)\n",
    "    download(path+'riv/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'riv',\n",
    "        ID)\n",
    "    if ID != '49' # there is no 49 for hillslope\n",
    "        download(path+'hill/',\n",
    "             http_path+'coastal_hillslopes/',\n",
    "            'hill',\n",
    "            ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction of two problematic shapes based on their COMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the two problemative shapefiles in the entire catchemtns\n",
    "list_id = [11040208,56045327]  # the COMID IDs that result in shp_std_hard to crash hole outside shell\n",
    "for ID in IDs:\n",
    "    shp_std_light(path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                  path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                  path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_fixed.shp',\n",
    "                 'COMID',\n",
    "                  0.0000001,\n",
    "                  list_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction of the shapfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the shapefiles for the catchemtns\n",
    "for ID in IDs:\n",
    "    shp_std_hard(path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1_hole.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1_log.txt',\n",
    "                 'COMID',\n",
    "                 0.0000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After correction of shapefile, there are still invalid shape based on QGIS validity checks, we add those shapefiles to be corrected once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the corrected shapefiles from the pervious block with different rules in QGIS with checkvalidity\n",
    "# is needed provide the COMID here to be recorrected\n",
    "list_id = [11038670,11040208,11035758,\n",
    "           12073970,\n",
    "           16008278,16009917,16012413,\n",
    "           17008507,\n",
    "           25000050,\n",
    "           28045843,28046799,28047182,28050769,28059551,28064206,\n",
    "           29020703,29028261,29034407,29048575,29050425,29071345,29092185,\n",
    "           31042597,31079633,32029132,\n",
    "           43031330,43050399,43063824,\n",
    "           45074597,\n",
    "           47014085,\n",
    "           48001834,48021477,48026003,\n",
    "           49010729,\n",
    "           52010007,\n",
    "           56141996,56158704,\n",
    "           61008413,61026810,\n",
    "           62038658,\n",
    "           64009361,64074730,\n",
    "           65027306,\n",
    "           66009603,\n",
    "           72055397,72055872,72058490,\n",
    "           74006369,74071283,\n",
    "           75027926,\n",
    "           77014997,\n",
    "           78012325,\n",
    "           81033705,\n",
    "           82004214,82041566,82002087,\n",
    "           91025753,91035154,91035236,91035911]  # list of COMID that are still not valid based on QGIS\n",
    "for ID in IDs:\n",
    "    shp_std_light(path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1.shp',\n",
    "                  path+ 'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2.shp',\n",
    "                  path+ 'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2_fixedshp.shp',\n",
    "                 'COMID',\n",
    "                 0.0000001,\n",
    "                 list_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hillslope correction\n",
    "### # there is no 49 hillslope and there is only one shape in hillslope 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = ['11', '12', '13', '14', '15', '16', '17', '18',\n",
    "       '21', '22', '23', '24', '25', '26', '27', '28', '29',\n",
    "       '31', '32', '33', '34', '35',\n",
    "       '41', '42', '43', '44', '45', '46', '47', '48',\n",
    "       '51', '52', '53', '54', '55', '56', '57',\n",
    "       '61', '62', '63', '64', '65', '66', '67',\n",
    "       '71', '72', '73', '74', '75', '76', '77', '78',\n",
    "       '81', '82', '83', '84', '85', '86',\n",
    "       '91']\n",
    "\n",
    "# break the hillslopes into separaete hillslopes between the river segments\n",
    "for ID in IDs:\n",
    "    shp_hill (path+'hill/hillslope_'+ID+'_clean.shp',\n",
    "              path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "              path+'hill_step_0/hillslope_'+ID+'_clean_corr1.shp',\n",
    "              0.0000001)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding the COMID to the decomposed unresolved hillslopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add COMID ID to modified hillslope\n",
    "for ID in IDs:\n",
    "    shp = gpd.read_file(path+'hill_step_0/hillslope_'+ID+'_clean_corr1.shp')\n",
    "    cat = gpd.read_file(path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp')\n",
    "    shp ['COMID'] = np.arange(shp.shape[0])+max(cat.COMID)+1\n",
    "    shp.to_file(path+'hill_fixed/hillslope_'+ID+'_clean_fixed.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intersection with ERA5 for candex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection\n",
    "IDs = ['71', '72', '73', '74', '75', '76', '77', '78',\n",
    "       '81', '82', '83', '84', '85', '86'] # IDs for north america\n",
    "# location where the ERA5 shapefile is located\n",
    "ERA5_filename = '/Users/shg096/Desktop/ERA_5/era5_land_withArea.shp'\n",
    "# dict for rename of the intersected shapefile\n",
    "dict_rename = {\"S_1_COMID\" : \"COMID\", # hruId that is used as SUMMA computational units ID\n",
    "              \"S_1_lat_c\" : \"lat\", # lon of hru\n",
    "              \"S_1_lon_c\" : \"lon\", # lat of hru\n",
    "              \"S_2_shp_ID\": \"ERA5ID\", # ERA5 grid ID, not used\n",
    "              \"S_2_lat\"   : \"ERA5lat\", # lon of forcing grid to be read\n",
    "              \"S_2_lon\"   : \"ERA5lon\", # lat of forcing grid to be read\n",
    "              \"AP1N\"      : \"ERA5W\"}\n",
    "for ID in IDs:\n",
    "    intersect(path+'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2.shp',\n",
    "              ERA5_filename,\n",
    "              path+ 'ERA5int/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2_ERA5.shp',\n",
    "              'COMID',\n",
    "              dict_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection for unresolved hillslopes\n",
    "IDs = ['71', '72', '73', '74', '75', '76', '77', '78',\n",
    "       '81', '82', '83', '84', '85', '86']  # IDs for north america\n",
    "# location where the ERA5 shapefile is located\n",
    "ERA5_filename = '/Users/shg096/Desktop/ERA_5/era5_land_withArea.shp'\n",
    "# dict for rename of the intersected shapefile\n",
    "dict_rename = {\"S_1_COMID\" : \"COMID\", # hruId that is used as SUMMA computational units ID\n",
    "              \"S_1_lat_c\" : \"lat\", # lon of hru\n",
    "              \"S_1_lon_c\" : \"lon\", # lat of hru\n",
    "              \"S_2_shp_ID\": \"ERA5ID\", # ERA5 grid ID, not used\n",
    "              \"S_2_lat\"   : \"ERA5lat\", # lon of forcing grid to be read\n",
    "              \"S_2_lon\"   : \"ERA5lon\", # lat of forcing grid to be read\n",
    "              \"AP1N\"      : \"ERA5W\"}\n",
    "for ID in IDs:\n",
    "    intersect(path+'hill_fixed/hillslope_'+ID+'_clean_fixed.shp',\n",
    "              ERA5_filename,\n",
    "              path+ 'ERA5int/hillslope_'+ID+'_clean_fixed_ERA5.shp',\n",
    "              'COMID',\n",
    "              dict_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add COMID ID to modified hillslope\n",
    "IDs = ['31', '41', '61','91'] # missing 31000002, 41000074, 61000009,91000024\n",
    "for ID in IDs:\n",
    "    cat = gpd.read_file(path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp')\n",
    "    cat = cat.iloc[cat['COMID'].sort_values().index.values]\n",
    "    cat = cat.reset_index()\n",
    "    cat ['COMID_N'] = cat['COMID'].shift(-1)\n",
    "    cat ['dif'] = cat['COMID']- cat['COMID_N']\n",
    "    plt.plot(cat['dif'])\n",
    "    print(cat.head(100))\n",
    "    for i in np.arange (1000):\n",
    "        print(cat.COMID.iloc[i],cat.dif.iloc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
