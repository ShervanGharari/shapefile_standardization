{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.ops import unary_union\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def remove_dublicate_point(poly):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "    \n",
    "    This function gets a shapely polygon and remove dublicated point there\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    poly: shapely polygon, the input shapely polygon\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \n",
    "    \"\"\"\n",
    "    A = 10000000\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    X  = np.floor(np.array(x)*A)/A\n",
    "    Y  = np.floor(np.array(y)*A)/A\n",
    "    X = np.transpose(X)\n",
    "    Y = np.transpose(Y)\n",
    "    df = pd.DataFrame({'X': X, 'Y': Y})\n",
    "    df = df.drop_duplicates()\n",
    "    df['geometry'] = df.apply(lambda row: Point(row.X, row.Y), axis=1)\n",
    "    poly = Polygon([(p.x, p.y)  for p in  df.geometry])\n",
    "    return poly\n",
    "\n",
    "\n",
    "def download(des_loc,\n",
    "            http_page,\n",
    "            str1,\n",
    "            str2):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a http and two str in the name of the link and save them in\n",
    "    provided destnation\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    des_loc: string, the name of the source file including path and extension\n",
    "    http_page: string, the name of the corresponding catchment (subbasin) for the unresolved hills\n",
    "    str1: string, a part of the link name to filter\n",
    "    str2: string, a second part of the link name to filter\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \"\"\"\n",
    "\n",
    "    # first get all the links in the page\n",
    "    req = Request(http_page)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    # specify the link to be downloaded\n",
    "    link_to_download = []\n",
    "    for link in links:\n",
    "        # if \"hillslope\" in link and \"clean\" in link: # links that have cat_pfaf and Basins in them\n",
    "        if str1 in link and str2 in link: # links that have cat_pfaf and Basins in them\n",
    "            link_to_download.append(link)\n",
    "            print(link)\n",
    "\n",
    "    # creat urls to download\n",
    "    urls =[]\n",
    "    for file_name in link_to_download:\n",
    "        urls.append(http_page+file_name) # link of the page + file names\n",
    "        print(http_page+file_name)\n",
    "    print(urls)\n",
    "\n",
    "    # loop to download the data\n",
    "    for url in urls:\n",
    "        name = url.split('/')[-1] # get the name of the file at the end of the url to download\n",
    "        r = requests.get(url) # download the URL\n",
    "        # print the specification of the download \n",
    "        print(r.status_code, r.headers['content-type'], r.encoding)\n",
    "        # if download successful the statuse code is 200 then save the file, else print not downloaded\n",
    "        if r.status_code == 200:\n",
    "            print('download was successful for '+url)\n",
    "            with open(des_loc+name, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        else:\n",
    "            print('download was not successful for '+url)\n",
    "\n",
    "\n",
    "def shp_hill (name_of_source_file, name_of_cat_file, name_of_result_file, epsilon):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile and remove inernal holes\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_cat_file: string, the name of the corresponding catchment (subbasin)\n",
    "        for the unresolved hills\n",
    "    name_of_result_file: string, the name of the file that includes fixed shapes\n",
    "        including path and extension\n",
    "    epsilon\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the fixed shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    shp = gpd.read_file(name_of_source_file)\n",
    "    cat1 = gpd.read_file(name_of_cat_file)\n",
    "\n",
    "\n",
    "    ## STEP1, load a shapefile, and find its intesection with itself. there are\n",
    "    ## there should be some holes in the shapefile. the holes are given as a separaete shape\n",
    "\n",
    "    shp_temp = gpd.overlay(shp, shp, how='intersection')\n",
    "    shp_temp = shp_temp [shp_temp.FID_1 != shp_temp.FID_2]\n",
    "    #shp_temp.to_file('temp1.shp')\n",
    "    shp_temp = gpd.overlay(shp, shp_temp, how='difference')\n",
    "    #shp_temp.to_file('temp2.shp')\n",
    "\n",
    "    ## STEP2, remove possible cat from the unresolved costal hillslope\n",
    "    shp_temp = gpd.overlay(shp_temp, cat1, how='difference')\n",
    "    #shp_temp.to_file('temp3.shp')\n",
    "\n",
    "    ## STEP3, break the touching polygons into separate polygons, remove the links (lines)\n",
    "    shp_temp = shp_temp.buffer(-epsilon).buffer(epsilon)\n",
    "    shp_temp.to_file('temp4.shp')\n",
    "\n",
    "    ## STEP4, break the polygones into separete shape in a shapefile\n",
    "    shp = gpd.read_file('temp4.shp')\n",
    "\n",
    "    shp_all = None\n",
    "\n",
    "    for index, _ in shp.iterrows():\n",
    "\n",
    "        polys = shp.geometry.iloc[index] # get the shape\n",
    "\n",
    "        if polys.type is 'Polygon':\n",
    "            shp_temp = gpd.GeoSeries(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            shp_temp = gpd.GeoDataFrame(shp_temp) # convert multipolygon to a shapefile with polygons\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "        if polys.type is 'MultiPolygon':\n",
    "            shp_temp = gpd.GeoDataFrame(polys) # convert multipolygon to a shapefile with polygons \n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "\n",
    "        if shp_all is None:\n",
    "            shp_all = shp_temp\n",
    "        else:\n",
    "            shp_all = shp_all.append(shp_temp)\n",
    "\n",
    "    for index, _ in shp_all.iterrows(): # assuming the code convert everything to polygone (and not multi)\n",
    "        poly = shp_all.geometry.iloc[index]\n",
    "        poly = remove_dublicate_point (poly)\n",
    "        shp_all.geometry.iloc[index] = poly\n",
    "\n",
    "    shp_all.to_file(name_of_result_file)\n",
    "\n",
    "\n",
    "def extract_poly_coords(geom):\n",
    "    if geom.type == 'Polygon':\n",
    "        exterior_coords = geom.exterior.coords[:]\n",
    "        interior_coords = []\n",
    "        for interior in geom.interiors:\n",
    "            interior_coords += interior.coords[:]\n",
    "    elif geom.type == 'MultiPolygon':\n",
    "        exterior_coords = []\n",
    "        interior_coords = []\n",
    "        for part in geom:\n",
    "            epc = extract_poly_coords(part)  # Recursive call\n",
    "            exterior_coords += epc['exterior_coords']\n",
    "            interior_coords += epc['interior_coords']\n",
    "    else:\n",
    "        raise ValueError('Unhandled geometry type: ' + repr(geom.type))\n",
    "    return {'exterior_coords': exterior_coords,\n",
    "            'interior_coords': interior_coords}\n",
    "\n",
    "\n",
    "def shp_std_light(name_of_source_file,\n",
    "              name_of_result_file,\n",
    "              name_of_result_file_fixed_shapes,\n",
    "              ID_field,\n",
    "              epsilon,\n",
    "              list_id):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile and remove inernal holes\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_result_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file_fixed_shapes: string, the name of the file that includes fixed shapes\n",
    "        including path and extension\n",
    "    name_of_log_file: string, the name of the text log file with path and txt extension\n",
    "    ID_field: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    epsilon: real, the minimum distance for buffer operation\n",
    "    list_id: list of shape IDs that should be corrected\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the fixed shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the shapefile\n",
    "    shp = gpd.read_file(name_of_source_file)\n",
    "    print(shp.shape[0])\n",
    "    shp_new = shp # pass the shape to a new shape\n",
    "    shp_new['flag'] = 0 # add flag for the shapefile ids that are resolved\n",
    "    \n",
    "    for ID in list_id:\n",
    "        for index, _ in shp.iterrows():\n",
    "            if shp[ID_field][index] == ID:\n",
    "                shp_temp = shp.geometry.iloc[index]\n",
    "                shp_temp = shp_temp.buffer(epsilon) # to amalgamate tmultipolygons into a polygon\n",
    "                shp_temp = gpd.GeoSeries(shp_temp) # to geoseries\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # to geoframe\n",
    "                shp_temp.columns = ['geometry'] # call the colomn geometry\n",
    "                poly = shp_temp.geometry.iloc[0] # get the polygone from the shapefile\n",
    "                A = extract_poly_coords(poly) # extract the exterior\n",
    "                outer = A['exterior_coords'] # pass the exterior\n",
    "                poly_new = Polygon (outer) # make a polygone out of the \n",
    "                shp_new.geometry.iloc[index] = poly_new # pass the geometry to the new shapefile\n",
    "                shp_new['flag'].iloc[index] = 1 # put flag as 1\n",
    "                shp_temp = shp_new.geometry.iloc[index] # get the shape\n",
    "                shp_temp = shp_temp.buffer(-epsilon) # redo the buffer\n",
    "                shp_temp = gpd.GeoSeries(shp_temp) # to geoseries\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # geo dataframe\n",
    "                shp_temp.columns = ['geometry'] # name the column as geometry\n",
    "                shp_new.geometry.iloc[index] = shp_temp.geometry.iloc[0] # pass that to the new shape\n",
    "                poly = shp_new.geometry.iloc[index]\n",
    "                poly = remove_dublicate_point (poly)\n",
    "                shp_new.geometry.iloc[index] = poly\n",
    "    \n",
    "    shp_new.to_file(name_of_result_file) \n",
    "    shp_new = shp_new [shp_new.flag ==1]\n",
    "    if not shp_new.empty:\n",
    "        shp_new.to_file(name_of_result_file_fixed_shapes)\n",
    "    \n",
    "#     shp_new = shp_new [shp_new.flag ==1] # get the shapes that flag are 1\n",
    "#     if not shp_new.empty:\n",
    "#         shp_new = shp_new.drop(columns=['flag'])\n",
    "#         shp_diff = gpd.overlay(shp, shp_new, how='difference') # get the difference\n",
    "#         shp_all = shp_new.append(shp_diff) # append the fixed shapefiles to the diff\n",
    "#         if shp.shape[0] == shp_all.shape[0]:\n",
    "#             shp_new.to_file(name_of_result_file_fixed_shapes) #saved the fixed shapefile\n",
    "#             shp_all.to_file(name_of_result_file) # save the entire\n",
    "#         else:\n",
    "#             print('input output have different lenght; check')\n",
    "\n",
    "def shp_std_hard(name_of_source_file,\n",
    "            name_of_result_file,\n",
    "            name_of_result_file_holes,\n",
    "            name_of_log_file,\n",
    "            ID_field,\n",
    "            area_tolerance):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile, its directory, and its extensions (such as gpkg or shp) and\n",
    "    save a stadard shapefile. if presence it also save the holes of a shapefile\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_result_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file_holes: string, the name of the file that includes holes including path\n",
    "        and extension\n",
    "    name_of_log_file: string, the name of the text log file with path and txt extension\n",
    "    ID_field: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    area_tolerance: float; the tolerance to compare area before and after correction and report\n",
    "        differences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the removed problematice holes\n",
    "    a log file in the same folder descringin the invalid shapefiles\n",
    "    \"\"\"\n",
    "\n",
    "    shp_original = gpd.read_file(name_of_source_file)\n",
    "    shp_poly     = shp_original\n",
    "    shp_hole     = None\n",
    "\n",
    "    logfile = open(name_of_log_file,\"w\") # preparing the log file to write\n",
    "\n",
    "    number_invalid = 0 # counter for invalid shapes\n",
    "    number_resolved = 0 # counter for resolved invalid shapes\n",
    "    number_not_resolved = 0 # counter for not resolved invalid shapes\n",
    "\n",
    "    for index, _ in shp_original.iterrows():\n",
    "\n",
    "        # initialization\n",
    "        polys = shp_original.geometry.iloc[index] # get the shape\n",
    "        area_before = polys.area # area before changes\n",
    "        invalid = False # initializing invalid as false\n",
    "        \n",
    "        # check if the shapefile is valid\n",
    "        if polys.is_valid is False: # check if the geometry is invalid\n",
    "            number_invalid = number_invalid + 1\n",
    "            invalid = True\n",
    "            str_temp = str(number_invalid)+\". shape with ID \"+str(shp_original[ID_field].iloc[index])+\\\n",
    "            \" is not valid\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "        # put the shape into a Polygon or MultiPolygon\n",
    "        if polys.type is 'Polygon':\n",
    "            # print(polys.type)\n",
    "            shp_temp = gpd.GeoSeries(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            #shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            shp_temp = gpd.GeoDataFrame(shp_temp) # convert multipolygon to a shapefile with polygons\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            #print(shp_temp)\n",
    "        if polys.type is 'MultiPolygon':\n",
    "            # print(polys.type)\n",
    "            shp_temp = gpd.GeoDataFrame(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            #print(shp_temp)\n",
    "\n",
    "        has_holes = False # initializing hole as false\n",
    "        shp_temp['CCW'] = 0 # initialize check for couterclockwise (holes)\n",
    "        for index1, _ in shp_temp.iterrows(): #loop over polygone of one element\n",
    "            poly = shp_temp.geometry.iloc[index1] # get the geometry of polygon\n",
    "            if poly.exterior.is_ccw is True: # then the polgone is a hole\n",
    "                shp_temp['CCW'].iloc[index1] = 1 # set the hole flag to 1\n",
    "                shp_temp['geometry'].iloc[index1] = shapely.geometry.polygon.orient(poly, sign = +1) \n",
    "                # +1 CCW\n",
    "                #print(shp_temp['geometry'].iloc[index1])\n",
    "                has_holes = True\n",
    "\n",
    "        shp_temp_polys = shp_temp[shp_temp.CCW ==0] # get the polyons that are not couter clockwise\n",
    "        shp_temp_polys['dis'] = 0 # add a field for desolve\n",
    "        shp_temp_polys = shp_temp_polys.dissolve(by='dis') # to one multipolygon\n",
    "        polys_temp = shp_temp_polys.geometry.iloc[0] # update the shapefile on that\n",
    "        polys_temp = unary_union(polys_temp) # unify all the polygons into a multipolygons\n",
    "        shp_poly.geometry.iloc[index] = polys_temp.buffer(0) # fix the issue by buffer(0)\n",
    "        area_after = shp_poly.geometry.iloc[index].area # area after changes\n",
    "\n",
    "        # check if the shapefile becomes valid\n",
    "        # check if the geometry #is invalid\n",
    "        if shp_poly.geometry.iloc[index].is_valid is True and invalid is True: \n",
    "            str_temp = \" and becomes valid \\n\"\n",
    "            logfile.write(str_temp)\n",
    "            number_resolved = number_resolved + 1\n",
    "        # check if the geometry is invalid\n",
    "        if shp_poly.geometry.iloc[index].is_valid is False and invalid is True:\n",
    "            str_temp = \" and does not become valid; please check the shape \\n\"\n",
    "            logfile.write(str_temp)\n",
    "            number_not_resolved = number_not_resolved + 1\n",
    "\n",
    "        if has_holes is True:\n",
    "            shp_temp_holes = shp_temp[shp_temp.CCW ==1]\n",
    "            shp_temp_holes['dis'] = 0\n",
    "            shp_temp_holes = shp_temp_holes.dissolve(by='dis') # to one multipolyno\n",
    "            shp_temp_holes[ID_field] = shp_original[ID_field].iloc[index]\n",
    "            if shp_hole is None:\n",
    "                shp_hole = shp_temp_holes\n",
    "            else:\n",
    "                shp_hole = gpd.GeoDataFrame( pd.concat([shp_hole, shp_temp_holes], ignore_index=True) )\n",
    "            str_temp = \"Shape has a hole \\n\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "        if abs(area_before-area_after)>area_tolerance: # tolernace can be different based on projection\n",
    "            str_temp = \"shape area changes abs(\"+str(area_before)+\"-\"+str(area_after)+\") = \"+\\\n",
    "            str(area_before-area_after)+\" \\n\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "\n",
    "    shp_poly.to_file(name_of_result_file)\n",
    "    if shp_hole is not None:\n",
    "        shp_hole.to_file(name_of_result_file_holes) #save any hole to check\n",
    "\n",
    "    str_temp = \"Total number of shapes = \"+str(shp_original.shape[0])+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of invalid shapes = \"+str(number_invalid)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of resolved invalid shapes = \"+str(number_resolved)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of not resolved invalid shapes = \"+str(number_not_resolved)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    logfile.close() # close the log gile\n",
    "\n",
    "def intersection_shp(shp_1, shp_2):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/candex\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  Apache2\n",
    "    This fucntion intersect two shapefile. It keeps the fiels from the first and second shapefiles (identified by S_1_ and \n",
    "    S_2_). It also creats other field including AS1 (area of the shape element from shapefile 1), IDS1 (an arbitary index\n",
    "    for the shapefile 1), AS2 (area of the shape element from shapefile 1), IDS2 (an arbitary index for the shapefile 1), \n",
    "    AINT (the area of teh intersected shapes), AP1 (the area of the intersected shape to the shapes from shapefile 1),\n",
    "    AP2 (the area of teh intersected shape to the shapefes from shapefile 2), AP1N (the area normalized in the case AP1\n",
    "    summation is not 1 for a given shape from shapefile 1, this will help to preseve mass if part of the shapefile are not \n",
    "    intersected), AP2N (the area normalized in the case AP2 summation is not 1 for a given shape from shapefile 2, this\n",
    "    will help to preseve mass if part of the shapefile are not intersected)\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    shp1: geo data frame, shapefile 1\n",
    "    shp2: geo data frame, shapefile 2\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result: a geo data frame that includes the intersected shapefile and area, percent and normalized percent of each shape\n",
    "    elements in another one\n",
    "    \"\"\"\n",
    "    # Calculating the area of every shapefile (both should be in degree or meters)\n",
    "    column_names = shp_1.columns\n",
    "    column_names = list(column_names)\n",
    "\n",
    "    # removing the geometry from the column names\n",
    "    column_names.remove('geometry')\n",
    "\n",
    "    # renaming the column with S_1\n",
    "    for i in range(len(column_names)):\n",
    "        shp_1 = shp_1.rename(\n",
    "            columns={column_names[i]: 'S_1_' + column_names[i]})\n",
    "\n",
    "    column_names = shp_2.columns\n",
    "    column_names = list(column_names)\n",
    "\n",
    "    # removing the geometry from the colomn names\n",
    "    column_names.remove('geometry')\n",
    "\n",
    "    # renaming the column with S_2\n",
    "    for i in range(len(column_names)):\n",
    "        shp_2 = shp_2.rename(\n",
    "            columns={column_names[i]: 'S_2_' + column_names[i]})\n",
    "\n",
    "    # Caclulating the area for shp1\n",
    "    shp_1['AS1'] = shp_1.area\n",
    "    shp_1['IDS1'] = np.arange(shp_1.shape[0])+1\n",
    "\n",
    "    # Caclulating the area for shp2\n",
    "    shp_2['AS2'] = shp_2.area\n",
    "    shp_2['IDS2'] = np.arange(shp_2.shape[0])+1\n",
    "\n",
    "    # making intesection\n",
    "    result = spatial_overlays (shp_1, shp_2, how='intersection')\n",
    "\n",
    "    # Caclulating the area for shp2\n",
    "    result['AINT'] = result['geometry'].area\n",
    "    result['AP1'] = result['AINT']/result['AS1']\n",
    "    result['AP2'] = result['AINT']/result['AS2']\n",
    "    \n",
    "    \n",
    "    # taking the part of data frame as the numpy to incread the spead\n",
    "    # finding the IDs from shapefile one\n",
    "    ID_S1 = np.array (result['IDS1'])\n",
    "    AP1 = np.array(result['AP1'])\n",
    "    AP1N = AP1 # creating the nnormalized percent area\n",
    "    ID_S1_unique = np.unique(ID_S1) #unique idea\n",
    "    for i in ID_S1_unique:\n",
    "        INDX = np.where(ID_S1==i) # getting the indeces\n",
    "        AP1N[INDX] = AP1[INDX] / AP1[INDX].sum() # normalizing for that sum\n",
    "        \n",
    "    # taking the part of data frame as the numpy to incread the spead\n",
    "    # finding the IDs from shapefile one\n",
    "    ID_S2 = np.array (result['IDS2'])\n",
    "    AP2 = np.array(result['AP2'])\n",
    "    AP2N = AP2 # creating the nnormalized percent area\n",
    "    ID_S2_unique = np.unique(ID_S2) #unique idea\n",
    "    for i in ID_S2_unique:\n",
    "        INDX = np.where(ID_S2==i) # getting the indeces\n",
    "        AP2N[INDX] = AP2[INDX] / AP2[INDX].sum() # normalizing for that sum\n",
    "        \n",
    "    result ['AP1N'] = AP1N\n",
    "    result ['AP2N'] = AP2N\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def spatial_overlays(df1, df2, how='intersection', reproject=True):\n",
    "    \"\"\"Perform spatial overlay between two polygons.\n",
    "    Currently only supports data GeoDataFrames with polygons.\n",
    "    Implements several methods that are all effectively subsets of\n",
    "    the union.\n",
    "    \n",
    "    Omer Ozak\n",
    "    ozak\n",
    "    https://github.com/ozak\n",
    "    https://github.com/geopandas/geopandas/pull/338\n",
    "    Parameters\n",
    "    ----------\n",
    "    df1 : GeoDataFrame with MultiPolygon or Polygon geometry column\n",
    "    df2 : GeoDataFrame with MultiPolygon or Polygon geometry column\n",
    "    how : string\n",
    "        Method of spatial overlay: 'intersection', 'union',\n",
    "        'identity', 'symmetric_difference' or 'difference'.\n",
    "    use_sindex : boolean, default True\n",
    "        Use the spatial index to speed up operation if available.\n",
    "    Returns\n",
    "    -------\n",
    "    df : GeoDataFrame\n",
    "        GeoDataFrame with new set of polygons and attributes\n",
    "        resulting from the overlay\n",
    "    \"\"\"\n",
    "    df1 = df1.copy()\n",
    "    df2 = df2.copy()\n",
    "    df1['geometry'] = df1.geometry.buffer(0)\n",
    "    df2['geometry'] = df2.geometry.buffer(0)\n",
    "    if df1.crs!=df2.crs and reproject:\n",
    "        print('Data has different projections.')\n",
    "        print('Converted data to projection of first GeoPandas DatFrame')\n",
    "        df2.to_crs(crs=df1.crs, inplace=True)\n",
    "    if how=='intersection':\n",
    "        # Spatial Index to create intersections\n",
    "        spatial_index = df2.sindex\n",
    "        df1['bbox'] = df1.geometry.apply(lambda x: x.bounds)\n",
    "        df1['sidx']=df1.bbox.apply(lambda x:list(spatial_index.intersection(x)))\n",
    "        pairs = df1['sidx'].to_dict()\n",
    "        nei = []\n",
    "        for i,j in pairs.items():\n",
    "            for k in j:\n",
    "                nei.append([i,k])\n",
    "        pairs = gpd.GeoDataFrame(nei, columns=['idx1','idx2'], crs=df1.crs)\n",
    "        pairs = pairs.merge(df1, left_on='idx1', right_index=True)\n",
    "        pairs = pairs.merge(df2, left_on='idx2', right_index=True, suffixes=['_1','_2'])\n",
    "        pairs['Intersection'] = pairs.apply(lambda x: (x['geometry_1'].intersection(x['geometry_2'])).buffer(0), axis=1)\n",
    "        pairs = gpd.GeoDataFrame(pairs, columns=pairs.columns, crs=df1.crs)\n",
    "        cols = pairs.columns.tolist()\n",
    "        cols.remove('geometry_1')\n",
    "        cols.remove('geometry_2')\n",
    "        cols.remove('sidx')\n",
    "        cols.remove('bbox')\n",
    "        cols.remove('Intersection')\n",
    "        dfinter = pairs[cols+['Intersection']].copy()\n",
    "        dfinter.rename(columns={'Intersection':'geometry'}, inplace=True)\n",
    "        dfinter = gpd.GeoDataFrame(dfinter, columns=dfinter.columns, crs=pairs.crs)\n",
    "        dfinter = dfinter.loc[dfinter.geometry.is_empty==False]\n",
    "        dfinter.drop(['idx1','idx2'], inplace=True, axis=1)\n",
    "        return dfinter\n",
    "    elif how=='difference':\n",
    "        spatial_index = df2.sindex\n",
    "        df1['bbox'] = df1.geometry.apply(lambda x: x.bounds)\n",
    "        df1['sidx']=df1.bbox.apply(lambda x:list(spatial_index.intersection(x)))\n",
    "        df1['new_g'] = df1.apply(lambda x: reduce(lambda x, y: x.difference(y).buffer(0), \n",
    "                                 [x.geometry]+list(df2.iloc[x.sidx].geometry)) , axis=1)\n",
    "        df1.geometry = df1.new_g\n",
    "        df1 = df1.loc[df1.geometry.is_empty==False].copy()\n",
    "        df1.drop(['bbox', 'sidx', 'new_g'], axis=1, inplace=True)\n",
    "        return df1\n",
    "    elif how=='symmetric_difference':\n",
    "        df1['idx1'] = df1.index.tolist()\n",
    "        df2['idx2'] = df2.index.tolist()\n",
    "        df1['idx2'] = np.nan\n",
    "        df2['idx1'] = np.nan\n",
    "        dfsym = df1.merge(df2, on=['idx1','idx2'], how='outer', suffixes=['_1','_2'])\n",
    "        dfsym['geometry'] = dfsym.geometry_1\n",
    "        dfsym.loc[dfsym.geometry_2.isnull()==False, 'geometry'] = dfsym.loc[dfsym.geometry_2.isnull()==False, 'geometry_2']\n",
    "        dfsym.drop(['geometry_1', 'geometry_2'], axis=1, inplace=True)\n",
    "        dfsym = gpd.GeoDataFrame(dfsym, columns=dfsym.columns, crs=df1.crs)\n",
    "        spatial_index = dfsym.sindex\n",
    "        dfsym['bbox'] = dfsym.geometry.apply(lambda x: x.bounds)\n",
    "        dfsym['sidx'] = dfsym.bbox.apply(lambda x:list(spatial_index.intersection(x)))\n",
    "        dfsym['idx'] = dfsym.index.values\n",
    "        dfsym.apply(lambda x: x.sidx.remove(x.idx), axis=1)\n",
    "        dfsym['new_g'] = dfsym.apply(lambda x: reduce(lambda x, y: x.difference(y).buffer(0), \n",
    "                         [x.geometry]+list(dfsym.iloc[x.sidx].geometry)) , axis=1)\n",
    "        dfsym.geometry = dfsym.new_g\n",
    "        dfsym = dfsym.loc[dfsym.geometry.is_empty==False].copy()\n",
    "        dfsym.drop(['bbox', 'sidx', 'idx', 'idx1','idx2', 'new_g'], axis=1, inplace=True)\n",
    "        return dfsym\n",
    "    elif how=='union':\n",
    "        dfinter = spatial_overlays(df1, df2, how='intersection')\n",
    "        dfsym = spatial_overlays(df1, df2, how='symmetric_difference')\n",
    "        dfunion = dfinter.append(dfsym)\n",
    "        dfunion.reset_index(inplace=True, drop=True)\n",
    "        return dfunion\n",
    "    elif how=='identity':\n",
    "        dfunion = spatial_overlays(df1, df2, how='union')\n",
    "        cols1 = df1.columns.tolist()\n",
    "        cols2 = df2.columns.tolist()\n",
    "        cols1.remove('geometry')\n",
    "        cols2.remove('geometry')\n",
    "        cols2 = set(cols2).intersection(set(cols1))\n",
    "        cols1 = list(set(cols1).difference(set(cols2)))\n",
    "        cols2 = [col+'_1' for col in cols2]\n",
    "        dfunion = dfunion[(dfunion[cols1+cols2].isnull()==False).values]\n",
    "        return dfunion\n",
    "\n",
    "def intersect(name_of_source_file,\n",
    "            name_of_ERA_file,\n",
    "            name_of_result_file,\n",
    "            field_ID,\n",
    "            dic_rename):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile, its directory, and its extensions (such as gpkg or shp) and\n",
    "    save a stadard shapefile. if presence it also save the holes of a shapefile\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_ERA_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file: string, the name of the file that includes holes including path\n",
    "        and extension\n",
    "    field_ID: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    dic_rename: float; the tolerance to compare area before and after correction and report\n",
    "        differences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the removed problematice holes\n",
    "    a log file in the same folder descringin the invalid shapefiles\n",
    "    \"\"\"\n",
    "    \n",
    "    shp1 = gpd.read_file (name_of_source_file)\n",
    "    shp1.crs = 'epsg:4326'\n",
    "    shp1[\"lon_c\"] = shp1.centroid.x # pass calculated centroid lon to the shp1\n",
    "    shp1[\"lat_c\"] = shp1.centroid.y # pass calculated centroid lat to the shp1\n",
    "    shp2 = gpd.read_file (name_of_ERA_file)\n",
    "    shp_int = intersection_shp(shp1, shp2)\n",
    "    shp_int = shp_int.rename(columns=dic_rename) # weight of each ERA5 grid in subbasin\n",
    "    shp_int = shp_int.sort_values(by=[field_ID])\n",
    "    shp_int.to_file(name_of_result_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-0, prepare the input\n",
    "# the 2 digit pfaf code for the shapefile to be processed\n",
    "\n",
    "IDs = ['21','22', '23', '24', '25','26','27','28','29']\n",
    "#IDs = ['21']\n",
    "IDs = ['86']\n",
    "http_path = 'http:XXX/'\n",
    "path = '/Users/shg096/Desktop/test/' # in this folder create subfolders cat, riv, hill, cat_step_1,\n",
    "                                    # cat_step_2\n",
    "ERA5_filename = '/Users/shg096/Desktop/ERA_5/era5_land_withArea.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP- prepare the folder and subfolders for download\n",
    "# path is the location were all the shapefiles and anupulaed shapfiles are saved\n",
    "# under path create five subfolders: cat, riv, hill, cat_step_0, cat_step_1, cat_fixed, hill_fixed\n",
    "if not os.path.exists(path+'cat'):\n",
    "    os.mkdir(path+'cat')\n",
    "if not os.path.exists(path+'riv'):\n",
    "    os.mkdir(path+'riv')\n",
    "if not os.path.exists(path+'hill'):\n",
    "    os.mkdir(path+'hill')\n",
    "if not os.path.exists(path+'cat_step_0'):\n",
    "    os.mkdir(path+'cat_step_0')\n",
    "if not os.path.exists(path+'cat_step_1'):\n",
    "    os.mkdir(path+'cat_step_1')\n",
    "if not os.path.exists(path+'cat_fixed'):\n",
    "    os.mkdir(path+'cat_fixed')\n",
    "if not os.path.exists(path+'ERA5int'):\n",
    "    os.mkdir(path+'ERA5int')\n",
    "if not os.path.exists(path+'hill_fixed'):\n",
    "    os.mkdir(path+'hill_fixed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg\n",
      "cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf\n",
      "cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp\n",
      "cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx\n",
      "['http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx']\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/cat_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx\n",
      "riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg\n",
      "riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf\n",
      "riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.prj\n",
      "riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp\n",
      "riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.prj\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx\n",
      "['http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.prj', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp', 'http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx']\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.cpg\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.dbf\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.prj\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shp\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/riv_pfaf_86_MERIT_Hydro_v07_Basins_v01_bugfix1.shx\n",
      "hillslope_86_clean.cpg\n",
      "hillslope_86_clean.dbf\n",
      "hillslope_86_clean.prj\n",
      "hillslope_86_clean.shp\n",
      "hillslope_86_clean.shx\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.cpg\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.dbf\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.prj\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.shp\n",
      "http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.shx\n",
      "['http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.cpg', 'http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.dbf', 'http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.prj', 'http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.shp', 'http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.shx']\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.cpg\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.dbf\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.prj\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.shp\n",
      "200 text/plain; charset=UTF-8 UTF-8\n",
      "download was successful for http://hydrology.princeton.edu/data/mpan/for_martyn/coastal_hillslopes/hillslope_86_clean.shx\n"
     ]
    }
   ],
   "source": [
    "# downlaod the catchment, river, costal hillslope\n",
    "for ID in IDs:\n",
    "    download(path+'cat/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'cat',\n",
    "        ID)\n",
    "    download(path+'riv/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'riv',\n",
    "        ID)\n",
    "    download(path+'hill/',\n",
    "         http_path+'coastal_hillslopes/',\n",
    "        'hill',\n",
    "        ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7281\n"
     ]
    }
   ],
   "source": [
    "# correct the two problemative shapefiles in the entire catchemtns\n",
    "list_id = [11040208,56045327]  # the COMID IDs that result in shp_std_hard to crash hole outside shell\n",
    "for ID in IDs:\n",
    "    shp_std_light(path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                  path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                  path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_fixed.shp',\n",
    "                 'COMID',\n",
    "                  0.0000001,\n",
    "                  list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the shapefiles for the catchemtns\n",
    "for ID in IDs:\n",
    "    shp_std_hard(path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1_hole.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1_log.txt',\n",
    "                 'COMID',\n",
    "                 0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7281\n"
     ]
    }
   ],
   "source": [
    "# check the corrected shapefiles from the pervious block with different rules in QGIS with checkvalidity\n",
    "# is needed provide the COMID here to be recorrected\n",
    "list_id = [11038670,11040208,11035758,\n",
    "           25000050,28045843,28046799,28047182,28050769,28059551,28064206,\n",
    "           29020703,29028261,29034407,29048575,29050425,29071345,29092185,\n",
    "           72055397,72055872,72058490,75027926,78012325,\n",
    "           81033705,82042214,82041566,82002087,\n",
    "           91025753,91035154,91035236,91035911]  # list of COMID that are still not valid based on QGIS\n",
    "for ID in IDs:\n",
    "    shp_std_light(path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1.shp',\n",
    "                  path+ 'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2.shp',\n",
    "                  path+ 'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2_fixedshp.shp',\n",
    "                 'COMID',\n",
    "                 0.0000001,\n",
    "                 list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the hillslopes into separaete hillslopes between the river segments\n",
    "for ID in IDs:\n",
    "    shp_hill (path+'hill/hillslope_'+ID+'_clean.shp',\n",
    "              path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "              path+'hill_fixed/hillslope_'+ID+'_clean.shp',\n",
    "              0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S_1_COMID  S_1_unitarea  S_1_flag  \\\n",
      "0      86000092     36.496161         0   \n",
      "1      86000024     52.949238         0   \n",
      "2      86000120     34.547795         0   \n",
      "3      86001300     96.298837         0   \n",
      "4      86001912     26.720203         0   \n",
      "...         ...           ...       ...   \n",
      "7276   86005493    133.819950         0   \n",
      "7277   86005492     40.109569         0   \n",
      "7278   86005490     36.347973         0   \n",
      "7279   86005489     70.739434         0   \n",
      "7280   86005488     12.226144         0   \n",
      "\n",
      "                                               geometry   S_1_lon_c  \\\n",
      "0     POLYGON ((-70.49542 83.07208, -70.49458 83.072...  -70.616327   \n",
      "1     POLYGON ((-75.69208 82.77958, -75.69125 82.779...  -75.592273   \n",
      "2     MULTIPOLYGON (((-79.75708 82.00708, -79.75625 ...  -79.410714   \n",
      "3     POLYGON ((-76.92958 81.28458, -76.92708 81.284...  -76.701400   \n",
      "4     POLYGON ((-76.88375 80.51792, -76.87625 80.517...  -76.700239   \n",
      "...                                                 ...         ...   \n",
      "7276  POLYGON ((-110.63208 75.39042, -110.63042 75.3... -110.437547   \n",
      "7277  POLYGON ((-110.93792 75.28792, -110.93792 75.2... -110.899095   \n",
      "7278  MULTIPOLYGON (((-85.65542 75.21958, -85.65458 ...  -85.560528   \n",
      "7279  MULTIPOLYGON (((-85.60542 75.29042, -85.60458 ...  -85.590817   \n",
      "7280  POLYGON ((-85.52708 75.40792, -85.52542 75.407...  -85.516607   \n",
      "\n",
      "      S_1_lat_c       AS1  IDS1  \n",
      "0     83.040419  0.024148     1  \n",
      "1     82.731170  0.033551     2  \n",
      "2     81.972026  0.019834     3  \n",
      "3     81.249953  0.050757     4  \n",
      "4     80.481112  0.012956     5  \n",
      "...         ...       ...   ...  \n",
      "7276  75.317038  0.042356  7277  \n",
      "7277  75.313744  0.012692  7278  \n",
      "7278  75.249032  0.011453  7279  \n",
      "7279  75.317792  0.022391  7280  \n",
      "7280  75.385867  0.003887  7281  \n",
      "\n",
      "[7281 rows x 8 columns]\n",
      "       S_2_shp_ID  S_2_lat  S_2_lon     S_2_gp_m     S_2_area  \\\n",
      "0             1.0    66.25  -169.75   150.513782  313344555.6   \n",
      "1             2.0    66.00  -169.75   128.474990  316435053.9   \n",
      "2             3.0    65.75  -169.75    74.427478  319519149.4   \n",
      "3             4.0    65.50  -169.75   -11.104021  322596781.8   \n",
      "4             5.0    63.75  -169.75   151.038515  343954219.7   \n",
      "...           ...      ...      ...          ...          ...   \n",
      "56919     56920.0    63.00   -50.50  1618.192335  353004515.1   \n",
      "56920     56921.0    62.75   -50.50   857.329302  356007131.6   \n",
      "56921     56922.0    62.50   -50.50   232.896882  359002577.0   \n",
      "56922     56923.0    62.25   -50.50    53.962887  361990793.8   \n",
      "56923     56924.0    62.00   -50.50    34.023028  364971724.4   \n",
      "\n",
      "                                                geometry     AS2   IDS2  \n",
      "0      POLYGON ((-169.87500 66.25000, -169.87500 66.3...  0.0625      1  \n",
      "1      POLYGON ((-169.87500 66.00000, -169.87500 66.1...  0.0625      2  \n",
      "2      POLYGON ((-169.87500 65.75000, -169.87500 65.8...  0.0625      3  \n",
      "3      POLYGON ((-169.87500 65.50000, -169.87500 65.6...  0.0625      4  \n",
      "4      POLYGON ((-169.87500 63.75000, -169.87500 63.8...  0.0625      5  \n",
      "...                                                  ...     ...    ...  \n",
      "56919  POLYGON ((-50.62500 63.00000, -50.62500 63.125...  0.0625  56920  \n",
      "56920  POLYGON ((-50.62500 62.75000, -50.62500 62.875...  0.0625  56921  \n",
      "56921  POLYGON ((-50.62500 62.50000, -50.62500 62.625...  0.0625  56922  \n",
      "56922  POLYGON ((-50.62500 62.25000, -50.62500 62.375...  0.0625  56923  \n",
      "56923  POLYGON ((-50.62500 62.00000, -50.62500 62.125...  0.0625  56924  \n",
      "\n",
      "[56924 rows x 8 columns]\n",
      "intersection done\n"
     ]
    }
   ],
   "source": [
    "dic_rename = {\"S_1_COMID\" : \"COMID\", # hruId that is used as SUMMA computational units ID\n",
    "              \"S_1_lat_c\" : \"lat\", # lon of hru\n",
    "              \"S_1_lon_c\" : \"lon\", # lat of hru\n",
    "              \"S_2_shp_ID\": \"ERA5ID\", # ERA5 grid ID, not used\n",
    "              \"S_2_lat\"   : \"ERA5lat\", # lon of forcing grid to be read\n",
    "              \"S_2_lon\"   : \"ERA5lon\", # lat of forcing grid to be read\n",
    "              \"AP1N\"      : \"ERA5W\"}\n",
    "for ID in IDs:\n",
    "    intersect(path+'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2.shp',\n",
    "              ERA5_filename,\n",
    "              path+ 'ERA5int/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2_ERA5.shp',\n",
    "              'COMID',\n",
    "              dic_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
