{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.ops import unary_union\n",
    "import shapely\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def remove_dublicate_point(poly):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "    \n",
    "    This function gets a shapely polygon and remove dublicated point there\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    poly: shapely polygon, the input shapely polygon\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \n",
    "    \"\"\"\n",
    "    A = 10000000\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    X  = np.floor(np.array(x)*A)/A\n",
    "    Y  = np.floor(np.array(y)*A)/A\n",
    "    X = np.transpose(X)\n",
    "    Y = np.transpose(Y)\n",
    "    df = pd.DataFrame({'X': X, 'Y': Y})\n",
    "    df = df.drop_duplicates()\n",
    "    df['geometry'] = df.apply(lambda row: Point(row.X, row.Y), axis=1)\n",
    "    poly = Polygon([(p.x, p.y)  for p in  df.geometry])\n",
    "    return poly\n",
    "\n",
    "\n",
    "def download(des_loc,\n",
    "            http_page,\n",
    "            str1,\n",
    "            str2):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a http and two str in the name of the link and save them in\n",
    "    provided destnation\n",
    "    \n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    des_loc: string, the name of the source file including path and extension\n",
    "    http_page: string, the name of the corresponding catchment (subbasin) for the unresolved hills\n",
    "    str1: string, a part of the link name to filter\n",
    "    str2: string, a second part of the link name to filter\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    downlaod the files from the websites and save them in the correct location\n",
    "    \"\"\"\n",
    "\n",
    "    # first get all the links in the page\n",
    "    req = Request(http_page)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    # specify the link to be downloaded\n",
    "    link_to_download = []\n",
    "    for link in links:\n",
    "        # if \"hillslope\" in link and \"clean\" in link: # links that have cat_pfaf and Basins in them\n",
    "        if str1 in link and str2 in link: # links that have cat_pfaf and Basins in them\n",
    "            link_to_download.append(link)\n",
    "            print(link)\n",
    "\n",
    "    # creat urls to download\n",
    "    urls =[]\n",
    "    for file_name in link_to_download:\n",
    "        urls.append(http_page+file_name) # link of the page + file names\n",
    "        print(http_page+file_name)\n",
    "    print(urls)\n",
    "\n",
    "    # loop to download the data\n",
    "    for url in urls:\n",
    "        name = url.split('/')[-1] # get the name of the file at the end of the url to download\n",
    "        r = requests.get(url) # download the URL\n",
    "        # print the specification of the download \n",
    "        print(r.status_code, r.headers['content-type'], r.encoding)\n",
    "        # if download successful the statuse code is 200 then save the file, else print not downloaded\n",
    "        if r.status_code == 200:\n",
    "            print('download was successful for '+url)\n",
    "            with open(des_loc+name, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        else:\n",
    "            print('download was not successful for '+url)\n",
    "\n",
    "\n",
    "def shp_hill (name_of_source_file, name_of_cat_file, name_of_result_file, epsilon):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile and remove inernal holes\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_cat_file: string, the name of the corresponding catchment (subbasin)\n",
    "        for the unresolved hills\n",
    "    name_of_result_file: string, the name of the file that includes fixed shapes\n",
    "        including path and extension\n",
    "    epsilon\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the fixed shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    shp = gpd.read_file(name_of_source_file)\n",
    "    cat1 = gpd.read_file(name_of_cat_file)\n",
    "\n",
    "\n",
    "    ## STEP1, load a shapefile, and find its intesection with itself. there are\n",
    "    ## there should be some holes in the shapefile. the holes are given as a separaete shape\n",
    "\n",
    "    shp_temp = gpd.overlay(shp, shp, how='intersection')\n",
    "    shp_temp = shp_temp [shp_temp.FID_1 != shp_temp.FID_2]\n",
    "    #shp_temp.to_file('temp1.shp')\n",
    "    shp_temp = gpd.overlay(shp, shp_temp, how='difference')\n",
    "    #shp_temp.to_file('temp2.shp')\n",
    "\n",
    "    ## STEP2, remove possible cat from the unresolved costal hillslope\n",
    "    shp_temp = gpd.overlay(shp_temp, cat1, how='difference')\n",
    "    #shp_temp.to_file('temp3.shp')\n",
    "\n",
    "    ## STEP3, break the touching polygons into separate polygons, remove the links (lines)\n",
    "    shp_temp = shp_temp.buffer(-epsilon).buffer(epsilon)\n",
    "    shp_temp.to_file('temp4.shp')\n",
    "\n",
    "    ## STEP4, break the polygones into separete shape in a shapefile\n",
    "    shp = gpd.read_file('temp4.shp')\n",
    "\n",
    "    shp_all = None\n",
    "\n",
    "    for index, _ in shp.iterrows():\n",
    "\n",
    "        polys = shp.geometry.iloc[index] # get the shape\n",
    "\n",
    "        if polys.type is 'Polygon':\n",
    "            shp_temp = gpd.GeoSeries(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            shp_temp = gpd.GeoDataFrame(shp_temp) # convert multipolygon to a shapefile with polygons\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "        if polys.type is 'MultiPolygon':\n",
    "            shp_temp = gpd.GeoDataFrame(polys) # convert multipolygon to a shapefile with polygons \n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "\n",
    "        if shp_all is None:\n",
    "            shp_all = shp_temp\n",
    "        else:\n",
    "            shp_all = shp_all.append(shp_temp)\n",
    "\n",
    "    for index, _ in shp_all.iterrows(): # assuming the code convert everything to polygone (and not multi)\n",
    "        poly = shp_all.geometry.iloc[index]\n",
    "        poly = remove_dublicate_point (poly)\n",
    "        shp_all.geometry.iloc[index] = poly\n",
    "\n",
    "    shp_all.to_file(name_of_result_file)\n",
    "\n",
    "\n",
    "def extract_poly_coords(geom):\n",
    "    if geom.type == 'Polygon':\n",
    "        exterior_coords = geom.exterior.coords[:]\n",
    "        interior_coords = []\n",
    "        for interior in geom.interiors:\n",
    "            interior_coords += interior.coords[:]\n",
    "    elif geom.type == 'MultiPolygon':\n",
    "        exterior_coords = []\n",
    "        interior_coords = []\n",
    "        for part in geom:\n",
    "            epc = extract_poly_coords(part)  # Recursive call\n",
    "            exterior_coords += epc['exterior_coords']\n",
    "            interior_coords += epc['interior_coords']\n",
    "    else:\n",
    "        raise ValueError('Unhandled geometry type: ' + repr(geom.type))\n",
    "    return {'exterior_coords': exterior_coords,\n",
    "            'interior_coords': interior_coords}\n",
    "\n",
    "\n",
    "def shp_std_light(name_of_source_file,\n",
    "              name_of_result_file,\n",
    "              name_of_result_file_fixed_shapes,\n",
    "              ID_field,\n",
    "              epsilon,\n",
    "              list_id):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile and remove inernal holes\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_result_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file_fixed_shapes: string, the name of the file that includes fixed shapes\n",
    "        including path and extension\n",
    "    name_of_log_file: string, the name of the text log file with path and txt extension\n",
    "    ID_field: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    epsilon: real, the minimum distance for buffer operation\n",
    "    list_id: list of shape IDs that should be corrected\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the fixed shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the shapefile\n",
    "    shp = gpd.read_file(name_of_source_file)\n",
    "    print(shp.shape[0])\n",
    "    shp_new = shp # pass the shape to a new shape\n",
    "    shp_new['flag'] = 0 # add flag for the shapefile ids that are resolved\n",
    "    \n",
    "    for ID in list_id:\n",
    "        for index, _ in shp.iterrows():\n",
    "            if shp[ID_field][index] == ID:\n",
    "                shp_temp = shp.geometry.iloc[index]\n",
    "                shp_temp = shp_temp.buffer(epsilon) # to amalgamate tmultipolygons into a polygon\n",
    "                shp_temp = gpd.GeoSeries(shp_temp) # to geoseries\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # to geoframe\n",
    "                shp_temp.columns = ['geometry'] # call the colomn geometry\n",
    "                poly = shp_temp.geometry.iloc[0] # get the polygone from the shapefile\n",
    "                A = extract_poly_coords(poly) # extract the exterior\n",
    "                outer = A['exterior_coords'] # pass the exterior\n",
    "                poly_new = Polygon (outer) # make a polygone out of the \n",
    "                shp_new.geometry.iloc[index] = poly_new # pass the geometry to the new shapefile\n",
    "                shp_new['flag'].iloc[index] = 1 # put flag as 1\n",
    "                shp_temp = shp_new.geometry.iloc[index] # get the shape\n",
    "                shp_temp = shp_temp.buffer(-epsilon) # redo the buffer\n",
    "                shp_temp = gpd.GeoSeries(shp_temp) # to geoseries\n",
    "                shp_temp = gpd.GeoDataFrame(shp_temp) # geo dataframe\n",
    "                shp_temp.columns = ['geometry'] # name the column as geometry\n",
    "                shp_new.geometry.iloc[index] = shp_temp.geometry.iloc[0] # pass that to the new shape\n",
    "                poly = shp_new.geometry.iloc[index]\n",
    "                poly = remove_dublicate_point (poly)\n",
    "                shp_new.geometry.iloc[index] = poly\n",
    "    \n",
    "    shp_new.to_file(name_of_result_file)\n",
    "    shp_new = shp_new [shp_new.flag ==1]\n",
    "    if not shp_new.empty:\n",
    "        shp_new.to_file(name_of_result_file_fixed_shapes)\n",
    "    \n",
    "#     shp_new = shp_new [shp_new.flag ==1] # get the shapes that flag are 1\n",
    "#     if not shp_new.empty:\n",
    "#         shp_new = shp_new.drop(columns=['flag'])\n",
    "#         shp_diff = gpd.overlay(shp, shp_new, how='difference') # get the difference\n",
    "#         shp_all = shp_new.append(shp_diff) # append the fixed shapefiles to the diff\n",
    "#         if shp.shape[0] == shp_all.shape[0]:\n",
    "#             shp_new.to_file(name_of_result_file_fixed_shapes) #saved the fixed shapefile\n",
    "#             shp_all.to_file(name_of_result_file) # save the entire\n",
    "#         else:\n",
    "#             print('input output have different lenght; check')\n",
    "\n",
    "def shp_std_hard(name_of_source_file,\n",
    "            name_of_result_file,\n",
    "            name_of_result_file_holes,\n",
    "            name_of_log_file,\n",
    "            ID_field,\n",
    "            area_tolerance):\n",
    "    \"\"\"\n",
    "    @ author:                  Shervan Gharari\n",
    "    @ Github:                  https://github.com/ShervanGharari/shapefile_standardization\n",
    "    @ author's email id:       sh.gharari@gmail.com\n",
    "    @license:                  MIT\n",
    "\n",
    "    This function gets name of a shapefile, its directory, and its extensions (such as gpkg or shp) and\n",
    "    save a stadard shapefile. if presence it also save the holes of a shapefile\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    name_of_source_file: string, the name of the source file including path and extension\n",
    "    name_of_result_file: string, the name of the final file including path and extension\n",
    "    name_of_result_file_holes: string, the name of the file that includes holes including path\n",
    "        and extension\n",
    "    name_of_log_file: string, the name of the text log file with path and txt extension\n",
    "    ID_field: string, the name of the field in the original shapefile that is used for keeping\n",
    "        track of holes\n",
    "    area_tolerance: float; the tolerance to compare area before and after correction and report\n",
    "        differences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "\n",
    "    Saves Files\n",
    "    -------\n",
    "    a shp file that includes corrected polygones\n",
    "    a possible shapefile that includes the removed problematice holes\n",
    "    a log file in the same folder descringin the invalid shapefiles\n",
    "    \"\"\"\n",
    "\n",
    "    shp_original = gpd.read_file(name_of_source_file)\n",
    "    shp_poly     = shp_original\n",
    "    shp_hole     = None\n",
    "\n",
    "    logfile = open(name_of_log_file,\"w\") # preparing the log file to write\n",
    "\n",
    "    number_invalid = 0 # counter for invalid shapes\n",
    "    number_resolved = 0 # counter for resolved invalid shapes\n",
    "    number_not_resolved = 0 # counter for not resolved invalid shapes\n",
    "\n",
    "    for index, _ in shp_original.iterrows():\n",
    "\n",
    "        # initialization\n",
    "        polys = shp_original.geometry.iloc[index] # get the shape\n",
    "        area_before = polys.area # area before changes\n",
    "        invalid = False # initializing invalid as false\n",
    "        \n",
    "        # check if the shapefile is valid\n",
    "        if polys.is_valid is False: # check if the geometry is invalid\n",
    "            number_invalid = number_invalid + 1\n",
    "            invalid = True\n",
    "            str_temp = str(number_invalid)+\". shape with ID \"+str(shp_original[ID_field].iloc[index])+\\\n",
    "            \" is not valid\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "        # put the shape into a Polygon or MultiPolygon\n",
    "        if polys.type is 'Polygon':\n",
    "            # print(polys.type)\n",
    "            shp_temp = gpd.GeoSeries(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            #shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            shp_temp = gpd.GeoDataFrame(shp_temp) # convert multipolygon to a shapefile with polygons\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            #print(shp_temp)\n",
    "        if polys.type is 'MultiPolygon':\n",
    "            # print(polys.type)\n",
    "            shp_temp = gpd.GeoDataFrame(polys) # convert multipolygon to a shapefile with polygons only\n",
    "            shp_temp.columns = ['geometry'] # naming geometry column\n",
    "            #print(shp_temp)\n",
    "\n",
    "        has_holes = False # initializing hole as false\n",
    "        shp_temp['CCW'] = 0 # initialize check for couterclockwise (holes)\n",
    "        for index1, _ in shp_temp.iterrows(): #loop over polygone of one element\n",
    "            poly = shp_temp.geometry.iloc[index1] # get the geometry of polygon\n",
    "            if poly.exterior.is_ccw is True: # then the polgone is a hole\n",
    "                shp_temp['CCW'].iloc[index1] = 1 # set the hole flag to 1\n",
    "                shp_temp['geometry'].iloc[index1] = shapely.geometry.polygon.orient(poly, sign = +1) \n",
    "                # +1 CCW\n",
    "                #print(shp_temp['geometry'].iloc[index1])\n",
    "                has_holes = True\n",
    "\n",
    "        shp_temp_polys = shp_temp[shp_temp.CCW ==0] # get the polyons that are not couter clockwise\n",
    "        shp_temp_polys['dis'] = 0 # add a field for desolve\n",
    "        shp_temp_polys = shp_temp_polys.dissolve(by='dis') # to one multipolygon\n",
    "        polys_temp = shp_temp_polys.geometry.iloc[0] # update the shapefile on that\n",
    "        polys_temp = unary_union(polys_temp) # unify all the polygons into a multipolygons\n",
    "        shp_poly.geometry.iloc[index] = polys_temp.buffer(0) # fix the issue by buffer(0)\n",
    "        area_after = shp_poly.geometry.iloc[index].area # area after changes\n",
    "\n",
    "        # check if the shapefile becomes valid\n",
    "        # check if the geometry #is invalid\n",
    "        if shp_poly.geometry.iloc[index].is_valid is True and invalid is True: \n",
    "            str_temp = \" and becomes valid \\n\"\n",
    "            logfile.write(str_temp)\n",
    "            number_resolved = number_resolved + 1\n",
    "        # check if the geometry is invalid\n",
    "        if shp_poly.geometry.iloc[index].is_valid is False and invalid is True:\n",
    "            str_temp = \" and does not become valid; please check the shape \\n\"\n",
    "            logfile.write(str_temp)\n",
    "            number_not_resolved = number_not_resolved + 1\n",
    "\n",
    "        if has_holes is True:\n",
    "            shp_temp_holes = shp_temp[shp_temp.CCW ==1]\n",
    "            shp_temp_holes['dis'] = 0\n",
    "            shp_temp_holes = shp_temp_holes.dissolve(by='dis') # to one multipolyno\n",
    "            shp_temp_holes[ID_field] = shp_original[ID_field].iloc[index]\n",
    "            if shp_hole is None:\n",
    "                shp_hole = shp_temp_holes\n",
    "            else:\n",
    "                shp_hole = gpd.GeoDataFrame( pd.concat([shp_hole, shp_temp_holes], ignore_index=True) )\n",
    "            str_temp = \"Shape has a hole \\n\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "        if abs(area_before-area_after)>area_tolerance: # tolernace can be different based on projection\n",
    "            str_temp = \"shape area changes abs(\"+str(area_before)+\"-\"+str(area_after)+\") = \"+\\\n",
    "            str(area_before-area_after)+\" \\n\"\n",
    "            logfile.write(str_temp)\n",
    "\n",
    "\n",
    "    shp_poly.to_file(name_of_result_file)\n",
    "    if shp_hole is not None:\n",
    "        shp_hole.to_file(name_of_result_file_holes) #save any hole to check\n",
    "\n",
    "    str_temp = \"Total number of shapes = \"+str(shp_original.shape[0])+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of invalid shapes = \"+str(number_invalid)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of resolved invalid shapes = \"+str(number_resolved)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    str_temp = \"Total number of not resolved invalid shapes = \"+str(number_not_resolved)+\" \\n\"\n",
    "    logfile.write(str_temp)\n",
    "    logfile.close() # close the log gile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-0, prepare the input\n",
    "# the 2 digit pfaf code for the shapefile to be processed\n",
    "\n",
    "IDs = ['21','22', '23', '24', '25','26','27','28','29']\n",
    "#IDs = ['21']\n",
    "http_path = 'http://hydrology.princeton.edu/data/mpan/for_martyn/'\n",
    "path = '/Users/shg096/Desktop/test/' # in this folder create subfolders cat, riv, hill, cat_step_1,\n",
    "                                    # cat_step_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP- prepare the folder and subfolders for download\n",
    "# path is the location were all the shapefiles and anupulaed shapfiles are saved\n",
    "# under path create five subfolders: cat, riv, hill, cat_step_0, cat_step_1, cat_fixed, hill_fixed\n",
    "if os.path.exists(path+'cat'):\n",
    "    os.mkdir(path+'cat')\n",
    "if os.path.exists(path+'riv'):\n",
    "    os.mkdir(path+'riv')\n",
    "if os.path.exists(path+'hill'):\n",
    "    os.mkdir(path+'hill')\n",
    "if os.path.exists(path+'cat_step_0'):\n",
    "    os.mkdir(path+'cat_step_0')\n",
    "if os.path.exists(path+'cat_step_1'):\n",
    "    os.mkdir(path+'cat_step_1')\n",
    "if os.path.exists(path+'cat_fixed'):\n",
    "    os.mkdir(path+'cat_fixed')\n",
    "if os.path.exists(path+'hill_fixed'):\n",
    "    os.mkdir(path+'hill_fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the catchment, river, costal hillslope\n",
    "for ID in IDs:\n",
    "    download(path+'cat/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'cat',\n",
    "        ID)\n",
    "    download(path+'riv/',\n",
    "         http_path+'MERIT_Hydro_v07_Basins_v01_bugfix1/pfaf_level_02/',\n",
    "        'riv',\n",
    "        ID)\n",
    "    download(path+'hill/',\n",
    "         http_path+'coastal_hillslopes/',\n",
    "        'hill',\n",
    "        ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the two problemative shapefiles in the entire catchemtns\n",
    "list_id = [11040208,56045327]  # the COMID IDs that result in shp_std_hard to crash hole outside shell\n",
    "for ID in IDs:\n",
    "    shp_std_light(path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                  path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                  path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_fixed.shp',\n",
    "                 'COMID',\n",
    "                  0.0000001,\n",
    "                  list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the shapefiles for the catchemtns\n",
    "for ID in IDs:\n",
    "    shp_std_hard(path+'cat_step_0/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1_hole.shp',\n",
    "                 path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1_log.txt',\n",
    "                 'COMID',\n",
    "                 0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the corrected shapefiles from the pervious block with different rules in QGIS with checkvalidity\n",
    "# is needed provide the COMID here to be recorrected\n",
    "list_id = [11038670,11040208,11035758,\n",
    "           25000050,28045843,28046799,28047182,28050769,28059551,28064206,\n",
    "           29020703,29028261,29034407,29048575,29050425,29071345,29092185,\n",
    "           72055397,72055872,72058490,75027926,78012325,\n",
    "           81033705,82042214,82041566,82002087,\n",
    "           91025753,91035154,91035236,91035911]  # list of COMID that are still not valid based on QGIS\n",
    "for ID in IDs:\n",
    "    shp_std_light(path+'cat_step_1/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr1.shp',\n",
    "                  path+ 'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2.shp',\n",
    "                  path+ 'cat_fixed/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1_corr2_fixedshp.shp',\n",
    "                 'COMID',\n",
    "                 0.0000001,\n",
    "                 list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the hillslopes into separaete hillslopes between the river segments\n",
    "for ID in IDs:\n",
    "    shp_hill (path+'hill/hillslope_'+ID+'_clean.shp',\n",
    "              path+'cat/cat_pfaf_'+ID+'_MERIT_Hydro_v07_Basins_v01_bugfix1.shp',\n",
    "              path+'hill_fixed/hillslope_'+ID+'_clean.shp',\n",
    "              0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp MERIT_Hydro_basin_bugfixed/cat_pfaf_*_MERIT_Hydro_v07_Basins_v01_bugfix1_valid_poly/*.* MERIT_Hydro_basin_bugfixed_valid/\n",
    "\n",
    "path = '/Users/shg096/Desktop/MERIT_Hydro_basin_bugfixed_valid/'\n",
    "basins = ['71','72','73','74','75','76','77','78','81','82','83','84','85','86'] # list of basins for NA and arctics\n",
    "\n",
    "for basin in basins:\n",
    "    shp1 = gpd.read_file (path+'cat_pfaf_'+basin+'_MERIT_Hydro_v07_Basins_v01_bugfix1_valid_poly.shp')\n",
    "    #shp1.crs = {'init': 'epsg:4326', 'no_defs': True}\n",
    "    shp1.crs = 'epsg:4326'\n",
    "    shp1[\"lon_c\"] = shp1.centroid.x # pass calculated centroid lon to the shp1\n",
    "    shp1[\"lat_c\"] = shp1.centroid.y # pass calculated centroid lat to the shp1\n",
    "    print(shp1.crs)\n",
    "    shp2 = gpd.read_file ('/Users/shg096/Desktop/era5_land_withArea.shp')\n",
    "    print(shp2.crs)\n",
    "    shp_int = intersection_shp(shp1, shp2)\n",
    "    shp_int = shp_int.rename(columns={\"S_1_COMID\" : \"COMID\", # hruId that is used as SUMMA computational units ID\n",
    "                                      \"S_1_lat_c\" : \"lat\", # lon of hru\n",
    "                                      \"S_1_lon_c\" : \"lon\", # lat of hru\n",
    "                                      \"S_2_shp_ID\": \"ERA5ID\", # ERA5 grid ID, not used\n",
    "                                      \"S_2_lat\"   : \"ERA5lat\", # lon of forcing grid to be read\n",
    "                                      \"S_2_lon\"   : \"ERA5lon\", # lat of forcing grid to be read\n",
    "                                      \"AP1N\"      : \"ERA5W\"}) # weight of each ERA5 grid in subbasin\n",
    "    shp_int = shp_int.sort_values(by=['COMID'])\n",
    "    shp_int.to_file(path+'cat_pfaf_'+basin+'_MERIT_Hydro_v07_Basins_v01_bugfix1_valid_poly_Era5.shp')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
